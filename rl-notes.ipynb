{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c52021",
   "metadata": {},
   "source": [
    "# Intro RL para LLMs\n",
    "\n",
    "- Agent/policy model: Estrategia para tomar decisiones. Es el LLM\n",
    "- Environment: provee feedback al agente. Puede ser muchas cosas aca\n",
    "- Action: Pueden ser PALABRAS, RESPUESTA ENTERA, OUTCOME FINAL, etc\n",
    "- Reward: Feedback que le da el env al agent. Numero\n",
    "\n",
    "---\n",
    "\n",
    "**RLHF**\n",
    "\n",
    "- Preferencias humanas: a partir de varias respuestas, las ranqueamos\n",
    "- Reward model: construimos un reward model basado en esas preferencias. Ahora tenemos un modelo que, dado una respuesta, estima el puntaje que le darian humanos\n",
    "- Fine-tune el LLM con RL: Ahora el reward model nos da el feedback. Entonces:\n",
    "    - El agente genera respuestas\n",
    "    - El reward model las scorea\n",
    "    - Ajustamos los weights para que tienda a producir outputs que sean bien puntuados\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**RL vs SL**\n",
    "\n",
    "Principalmente cambia la fuente de la se√±al (reward). Qu√© optimizas realmente\n",
    "- SL: Minimizar divergencia respecto a la distribuci√≥n objetivo (imitar). Tiene un reward **denso** porque sabes la respuesta target para cada token.\n",
    "- RL: Maximizar retorno esperado bajo din√°micas del entorno (elegir). La se√±al es escasa y esta diferida en el tiempo. Se requiere **exploracion** y credito temporal. \n",
    "\n",
    "Por eso se dice que SFT memorizes y RL generalizes https://arxiv.org/abs/2501.17161\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89482550",
   "metadata": {},
   "source": [
    "# Policy optimization (gradient-based)\n",
    "\n",
    "Tecnicas para optimizar el LLM / policy model para que mejore las predicciones.\n",
    "\n",
    "**Intro**:\n",
    "\n",
    "En SL tenemos una funcion de perdida (loss) y lo que queremos es minimizar esa loss. Entonces dado un batch, calculamos la loss (un numero final) basado en muchos errores (1 o varios para cada elemento del batch), los promediamos y nos queda ese numero final. Ahi, buscamos el gradiente (el conjunto de derivadas parciales de cada uno de los parametros aprendibles) que hagan que mas baje la loss en ese punto. Y actualizamos los valores de los parametros en la direccion y fuerza de esas derivadas y agregandole otros empujones.\n",
    "\n",
    "En RL, lo que tratamos de hacer es subir o bajar la probabilidad de los tokens segun si aparecen en la secuencia que genero un buen o mal resultado (bien o mal comparado con un baseline).\n",
    "\n",
    "\n",
    "**Pasos general (on-policy)**:\n",
    "- Se hacen rollouts (el modelo genera respuestas completas)\n",
    "- Se evalua cada respuesta y se obtiene score o reward\n",
    "- El score se convierte en ventaje (advantage): cuanto mejor o peor esta respuesta fue a lo esperado o tipico (por ej del grupo o de algo de referencia).\n",
    "    - Si tengo solo reward final, el advantage se aplica a todos los tokens de la secuencia de la misma manera (credito global)\n",
    "    - Si tengo verificadores por paso, se reparten ventajas distintas por token\n",
    "- Distribuyo la ventaja para cada token:\n",
    "    - Si es positiva, subo su log-prob (empujo logits para arriba).\n",
    "    - Si es negativa, la bajo.\n",
    "    - Indirectamnete afecto a las otras (porque en probs la suma = 1)\n",
    "- Regularizo para no irme tan lejos de una distribucion anterior (basada en una politica de referencia) para evitar colapso.\n",
    "\n",
    "**Caso general**\n",
    "- Nosotros tenemos una red y para un estado (secuenci), produce logits (next token / action)\n",
    "- Los pasamos por softmax y tenemos probs y de esos tomamos el log, por lo que trabajamos con log probs. \n",
    "- Para el token elegido en un paso t, vemos el log prob (solo tenemos un log prob en ese paso)\n",
    "- Si tomamos el gradiente de ese log prob, estamos viendo un conjunto de derivadas parciales del logprob con respecto a todos los parametros del modelo. Esto indica la direccion de MAXIMA SUBIDA -> o sea, si tomamos un paso en esa direccion, aumentamos la logprob de ese token para la prox.\n",
    "- Si hacemos lo mismo para cada token de la secuencia de pasos generados, y sumamos los gradientes (sumamos las derivadas parciales (o sea, para un parametro miramos las derivadas parciales por cada token en la secuencia generada y los sumamos)), nos queda un gradiente unico que apunta a maximizar todos los tokens en la secuencia generada. \n",
    "- Despues vemos el reward para esa secuencia y lo comparamos con el baseline (R-b). Si el baseline es 0.5 y nuestro reward es 0.2, el advantage da -0.3. Eso lo usamos para multiplicar, lo que nos cambia la direccion del gradiente si el advantage es negativo y tambien nos da una magnitud de cambio (escala el tama√±o del paso junto con el lr). O sea que si es negativo, en vez de apuntar a aumentar la prob, apunta a disminuir la prob de los tokens.\n",
    "- Para no favorecer secuencias largas, a veces se promedia la suma de logprobs y se suele agregar entropia (para mantener diversidad) y KL a un modelo de referencia para regularizar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a044d675",
   "metadata": {},
   "source": [
    "## REINFORCE\n",
    "\n",
    "Loss function: \n",
    "\n",
    "$$\\text L_\\theta = - (R - b) \\sum_t \\log \\pi_\\theta(a_t \\mid s_t)$$\n",
    "\n",
    "- Partimos de Maximum Likelihood: queremos que el modelo asigne alta probabilidad a lo que efectivamente ocurri√≥ (los datos reales).\n",
    "- Para secuencias, eso se traduce en un producto de probabilidades de cada token de la secuencia.\n",
    "- Como trabajar con productos es num√©ricamente inestable (tienden a 0), usamos logaritmo: eso convierte el producto en suma.\n",
    "- Advantage ùê¥=ùëÖ‚àíùëè: En RL, no todas las muestras valen lo mismo: algunas tuvieron mejor reward. Por eso ponderamos la suma con el advantage. El Advantage puede ser por paso (cuando haya reward por paso u otra tecnica) o general para la secuencia (para una secuencia particular, con respecto a otras (baseline)). Se puede poner dentro de la suma.\n",
    "\n",
    "$$\\text L_\\theta = - \\sum_t A_t \\cdot \\log \\pi_\\theta(a_t \\mid s_t)$$\n",
    "\n",
    "- La loss depende de Œ∏. Como regla de gradient descent tenemos: $\\theta \\leftarrow \\theta - a \\cdot \\nabla_\\theta L$.\n",
    "- Queremos derivar la loss respecto a Œ∏, modificando los pesos (que es lo unico que podemos mover) para que la loss sea mas chica.\n",
    "- Tenemos la politica (que es el LLM), que esto lo podemos mover, porque depende de Œ∏. Entonces lo queremos derivar: $\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)$\n",
    "- El gradiente de una funcion siempre nos dice el steepest ascent de la funcion con respecto a un parametro.\n",
    "- Si tenemos una Loss, vamos a querer derivarla respecto a los parametros del modelo. Entonces buscamos en la formula aquello que dependa de los parametros del modelo (pi)\n",
    "\n",
    "Entonces, el gradiente para la secuencia entera seria:\n",
    "\n",
    "$$\\nabla_\\theta L_\\theta = - \\sum_t A_t \\cdot \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)$$\n",
    "\n",
    "Este es el vector del gradiente, que apunta a donde mas crece L.\n",
    "\n",
    "Y en cuanto a gradient descent tenemos que, en cada optimizacion, theta cambia asi:\n",
    "\n",
    "$$\\Delta \\theta = - a \\cdot \\nabla_\\theta L(\\theta)$$\n",
    "\n",
    "O sea: \n",
    "\n",
    "$$\\theta \\leftarrow \\theta - a \\cdot \\nabla_\\theta L(\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ff1132a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0604,  0.0811, -0.0045],\n",
       "        [ 0.0880,  0.1048, -0.0045],\n",
       "        [-0.0723,  0.2866, -0.0566],\n",
       "        [ 0.0160, -0.0025,  0.1074],\n",
       "        [ 0.2263, -0.0918, -0.0225]], requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "vocab_size=5\n",
    "T=3\n",
    "lr=0.5\n",
    "\n",
    "# Model: linear map from one-hot(state) to vocab logits (bias-free for clarity).\n",
    "model = nn.Linear(T, vocab_size, bias=False)\n",
    "# Initialize weights small & reproducible\n",
    "with torch.no_grad():\n",
    "    model.weight.copy_(0.1 * torch.randn(vocab_size, T))\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "142af8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inputs: one-hot vectors for each time step 0..T-1\n",
    "I = torch.eye(T)  # shape (T, T)\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8794e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, -1.750232458114624, 0.17373354732990265),\n",
       " (1, -1.5883560180664062, 0.20426113903522491),\n",
       " (4, -1.6373724937438965, 0.19449038803577423)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CASO 1 (GOOD)\n",
    "chosen_tokens = [2, 1, 4]\n",
    "advantage = 0.4\n",
    "\n",
    "# --- Forward BEFORE update: record chosen tokens' log-probs and probs ---\n",
    "with torch.no_grad():\n",
    "    before = []\n",
    "    for t in range(T):\n",
    "        logits_t = model(I[t])          # (vocab,)\n",
    "        logp_t  = log_softmax(logits_t) # (vocab,)\n",
    "        tok = chosen_tokens[t]\n",
    "        before.append((tok, float(logp_t[tok]), float(logp_t.exp()[tok])))\n",
    "before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca91b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0: token 2 logprob=-1.750232458114624 total_logprob=-1.750232458114624\n",
      "t=1: token 1 logprob=-1.5883560180664062 total_logprob=-3.3385884761810303\n",
      "t=2: token 4 logprob=-1.6373724937438965 total_logprob=-4.975960731506348\n",
      "-advantage=-0.4\n",
      "-advantage * total_logprob = -0.4 - -4.975960731506348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.9904, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Compute REINFORCE loss:  L = -A * sum_t log pi(a_t | s_t) ---\n",
    "total_logprob = 0.0\n",
    "for t in range(T):\n",
    "    # sumamos todos los logprobs de los tokens elegidos\n",
    "    logits_t = model(I[t])          # (vocab,)\n",
    "    logp_t  = log_softmax(logits_t) # (vocab,)\n",
    "    tok = chosen_tokens[t]\n",
    "    total_logprob = total_logprob + logp_t[tok]\n",
    "    print(f't={t}: token {tok} logprob={logp_t[tok]} total_logprob={total_logprob}')\n",
    "print(f'-advantage={-advantage}\\n-advantage * total_logprob = {-advantage} - {total_logprob}')\n",
    "loss = - advantage * total_logprob\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c798d9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0207,  0.0412, -0.0441],\n",
       "        [ 0.0472,  0.2640, -0.0441],\n",
       "        [ 0.0930,  0.2376, -0.0941],\n",
       "        [-0.0219, -0.0392,  0.0631],\n",
       "        [ 0.1794, -0.1253,  0.1386]], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Backprop + step\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "opt.step()\n",
    "\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2c443e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-step chosen token probabilities (before -> after):\n",
      "  t=0: token=2   P_before=0.1737 -> P_after=0.2055   (‚Üë)\n",
      "  t=1: token=1   P_before=0.2043 -> P_after=0.2386   (‚Üë)\n",
      "  t=2: token=4   P_before=0.1945 -> P_after=0.2280   (‚Üë)\n"
     ]
    }
   ],
   "source": [
    "# --- Forward AFTER update: record chosen tokens' log-probs and probs ---\n",
    "with torch.no_grad():\n",
    "    after = []\n",
    "    for t in range(T):\n",
    "        logits_t = model(I[t])          # (vocab,)\n",
    "        logp_t  = log_softmax(logits_t) # (vocab,)\n",
    "        tok = chosen_tokens[t]\n",
    "        after.append((tok, float(logp_t[tok]), float(logp_t.exp()[tok])))\n",
    "\n",
    "print(\"Per-step chosen token probabilities (before -> after):\")\n",
    "for t, ((tok_b, logpb, pb), (tok_a, logpa, pa)) in enumerate(zip(before, after)):\n",
    "    assert tok_b == tok_a\n",
    "    arrow = \"‚Üë\" if pa > pb else (\"‚Üì\" if pa < pb else \"‚Üí\")\n",
    "    print(f\"  t={t}: token={tok_b}   P_before={pb:.4f} -> P_after={pa:.4f}   ({arrow})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa8513b",
   "metadata": {},
   "source": [
    "## PPO\n",
    "\n",
    "Limita cuanto se puede cambiar la distribucion para que no hayan pasos demasiado grandes y colapse.\n",
    "\n",
    "Hay un ratio por token\n",
    "\n",
    "$$r_t = \\frac{\\pi_{\\text{nueva}}(a_t \\mid s_t)}{\\pi_{\\text{vieja}}(a_t \\mid s_t)}$$\n",
    "\n",
    "Que te dice cuanto cambiaste la prob para un token. Si es 1.3, subiste 30% la probabilidad para ese token. Si es 0.7 bajaste 30% la prob para ese token.\n",
    "\n",
    "- **Clipping**: Si el paso te lleva a un r_t demasiado grande > 1 + eps, entonces recorto el beneficio\n",
    "- **KL divergence a un modelo referencia**: Penaliza alejarte del modelo base de referencia (el original) en promedio.\n",
    "- **Ventaja por paso** (opcional): en vez de tener un unico A global, lo hacemos por paso, con V(s_t) para asignar mejor el credito y premiar a los tokens clave. Si no esta, se puede usar como baseline la media del batch.\n",
    "- **Entropia**: bonus que mantiene la exploracion y evita colapso de entropia (que se vuelva modo unico)\n",
    "\n",
    "**PPO Loss**:\n",
    "\n",
    "$$L = \\mathbb{E}\\left[ \\min\\left(r_t A_t, \\; \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A_t \\right) \\right] - \\beta \\, \\mathrm{KL}(\\pi_{\\text{new}} \\| \\pi_{\\text{ref}}) + \\alpha \\, \\text{Entrop√≠a}$$\n",
    "\n",
    "Tres grandes partes:\n",
    "\n",
    "- Politica (clipped surrogate objective) -> $\\mathbb{E}\\left[ \\min\\left(r_t A_t, \\; \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A_t \\right) \\right]$\n",
    "- KL -> $- \\beta \\, \\mathrm{KL}(\\pi_{\\text{new}} \\| \\pi_{\\text{ref}})$\n",
    "- Entropia -> $\\alpha \\, \\text{Entrop√≠a}$\n",
    "\n",
    "\n",
    "**Gradiente**:\n",
    "\n",
    "- Lo mismo que REINFORCE, pensamos: Queremos minimizar esta loss. Como hacemos? Que depende de nosotros? los theta. Y donde estan, en pi_new. Todo lo demas es constante porque no depende de nosotros.\n",
    "- Donde esta pi_new? En el ratio: \n",
    "$$r_t = \\frac{\\pi_{\\text{new}}(a_t \\mid s_t)}{\\pi_{\\text{old}}(a_t \\mid s_t)} = \\exp\\left( \\log \\pi_{\\text{new}} - \\log \\pi_{\\text{old}} \\right)$$\n",
    "- Por eso al derivar, todo fluye por pi_new.\n",
    "- PROPIEDAD por la exp: la derivada del ratio es el propio ratio multiplicado por la derivada del log-prob nuevo:\n",
    "\n",
    "$$\\nabla_\\theta r_t = r_t \\cdot \\nabla_\\theta \\log \\pi_{\\text{new}}(a_t \\mid s_t)$$\n",
    " \n",
    "$$\\nabla_\\theta (\\exp\\left( \\log \\pi_{\\text{new}} - \\log \\pi_{\\text{old}} \\right)) = (\\exp\\left( \\log \\pi_{\\text{new}} - \\log \\pi_{\\text{old}} \\right)) \\cdot \\nabla_\\theta \\log \\pi_{\\text{new}}$$\n",
    "\n",
    "()\n",
    "\n",
    "Esto se da porque cuando tenemos una funcion exp:\n",
    "\n",
    "$y = \\exp(g(\\theta))$\n",
    "\n",
    "Al derivar exp, te devuelve exp otra vez, y despues multiplicas por la derivada de lo de adentro (**chain rule**):\n",
    "\n",
    "$\\nabla_\\theta y = y \\cdot \\nabla_\\theta g(\\theta)$\n",
    "\n",
    "- pi_old no esta porque no depende de theta, es constante -> $\\exp(\\log \\pi_{\\text{new}} - \\text constante) $\n",
    "- Entonces, cuando r_t = 1 (al principio cuando las dos policies son iguales), entonces el gradiente es igual a REINFORCE!\n",
    "\n",
    "**Politica (clipped surrogate objective)**:\n",
    "\n",
    "En REINFORCE tenemos A_t que es un empuje al gradiente: $A_t \\cdot \\nabla_\\theta \\pi$:\n",
    "- Grad de log pi: es la flecha (vector de cambios) que te dice como mover los pesos para aumentar el valor de la funcion.\n",
    "- A: Empuja a favor de subir o bajar la prob, segun el reward. Le puede cambiar el signo (si fue malo) y cambiarle intensidad.\n",
    "    - Si A > 0: queremos subir la prob\n",
    "    - Si A < 0: queremos bajar la prob\n",
    "\n",
    "En PPO tenemos lo mismo pero tambien el ratio:\n",
    "- A la vez de empujarlo con A_t, tambien lo empujamos por el ratio.\n",
    "- Esto es porque vimos que el grad queda: $\\nabla_\\theta r_t = r_t \\cdot \\nabla_\\theta \\log \\pi_{\\text{new}}$\n",
    "- Entonces, los empujes PPO quedarian: $A_t \\cdot r_t \\cdot \\nabla_\\theta \\log \\pi_{\\text{new}}$\n",
    "- Entonces, APARTE DEL **A_t**, ahora tambien lo escalamos por **r_t**: \n",
    "    - Si r_t == 1: queda igual a REINFORCE\n",
    "    - Si r_t > 1: la pi_new ya hace mas probable esa accion que pi_old.\n",
    "    - Si r_t < 1: la pi_new ya hace menos probable esa accion que pi_old.\n",
    "- EFECTOS DE ESCALAMIENTO DE GRADIENTES (interaccion A_t, r_t): -> NO LO TENGO SEGURO, NO ESTA CLARO\n",
    "    - Si A > 0, queremos aumentar la prob de esa accion.\n",
    "        - Si r_t > 1 (ej 1.6), potenciamos mas ese cambio. Decimos: ya venimos considerandola mas probable que el old, y ahora tambien lo queremos hacer mas probable, entonces le damos empujon mas grande (confianza?)\n",
    "        - Si r_t < 1 (ej 0.5), reducimos el tama√±o del cambio. Decimos: veniamos considerandola menos probable que el old, y ahora queremos hacerlo mas probable, entonces es mas sospechoso, hay que ser mas cautos? Estas son dudas, no lo tengo tan claro.\n",
    "    - Si A < 0, queremos bajar la prob de esa accion.\n",
    "        - Si r_t > 1 (ej 1.6), potenciamos mas ese cambio. Decimos: lo queremos menos probable y veniamos considerandolo mas probable, entonces lo bajamos mas fuerte...\n",
    "        - Si r_t < 1 (ej 0.5), reducimos el tama√±o del cambio. Decimos: lo queremos menos probable y veniamos considerandolo menos probable, pero ahora lo bajamos menos fuerte... ???\n",
    "\n",
    "**PPO Off-policy? - importance sampling (r_t)** (correccion estadistica):\n",
    "- Las trayectorias se recolectaron con pi_old (porque es caro volver a samplear), pero quer√©s optimizar como si vinieran de pi_new. Estamos trabajando con samples **out of distribution (off-policy)** -> Si bien PPO medio que se clasifica como on-policy en general, esta parte es off-policy.\n",
    "- En importance sampling, cuando queremos estimar una distribucion objetivo q(x) pero mis datos provienen de otra distribucion p(x), se suele reponderar cada muestra con un peso w(x)=q(x)/p(x).\n",
    "- La intuicion es corregir REPRESENTATIVIDAD, las caracteristicas que tengan dos distribuciones. Estamos calculando un update como  si mis datos vinieran de pi_new pero vienen de pi_old. Si no reponderamos, el estimador esta sesgado hacia pi_old. Si para un token en un step particular, con pi_old salia con prob 0.1 y con pi_new sale con 0.2... r = 0.2/0.1 = 2. Usando el mismo batch viejo, buscamos que el promedio o gradiente IMITE el que habria obtenido si hubiera generado el batch con pi_new. No es ‚Äúarbitrario‚Äù subir al doble ‚Äúese token‚Äù: es lo necesario para que la suma total refleje cu√°ntas veces deber√≠a aparecer ese token si hubieras muestreado con pi_new.\n",
    "- El **ratio arregla eso, es un reescalado**. queremos ‚Äúarreglar el PROMEDIO‚Äù.\n",
    "El update que buscamos en PPO es un promedio bajo la distribuci√≥n nueva pi_new. Pero tus datos vienen de pi_old. El √∫nico modo de que la suma de gradientes en el batch imite ese promedio ‚Äúcomo si‚Äù hubieras muestreado con pi_new es escalar cada muestra por el ratio.\n",
    "- Si multiplic√°s cada muestra por r = pi_new/pi_old, el promedio sobre tus datos viejos reproduce el promedio que tendr√≠as con datos nuevos. \n",
    "- **El gradiente objetivo es el promedio como si hubieras muestreado con new**: \n",
    "\n",
    "CLIP en PPO:\n",
    "Importance sampling puede dar pesos extremos (alta varianza). PPO mantiene la correcci√≥n pero pone guardarra√≠les: si r_t se va fuera de [1-œµ, 1+œµ] en la direcci√≥n que ‚Äúconviene‚Äù a A_t, corta el empuje (grad = 0 para ese token en ese minibatch).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**FLUJO**\n",
    "\n",
    "Los policies new NO SAMPLEAN. Lo unico que samplea es el OLD (se va actualizando cada tanto).\n",
    "\n",
    "- Tenemos un modelo pi_old. Tenemos prompts. Creamos minibatches (por ejemplo 4 prompts en cada minibatch) y tenemos 3 minibatches (12 ejemplos en total). \n",
    "- Sampleamos la respuesta usando pi_old y guardamos toda la info (probs de los tokens para CADA step, reward, etc) para cada prompt en todos los minibatches.\n",
    "- Ahora empieza la primera corrida de varios EPOCH (1 epoch = 1 pasada por todos los minibatches). Esto se va a hacer SIN HACER NINGUN SAMPLEO A MODELOS. Solo con los datos que estan.\n",
    "- Agarramos el primer minibatch de 4 ejemplos. Para cada ejemplo ya tenemos la secuencia/respuesta originada por el pi_old. Agarramos el primer ejemplo y vamos token por token de la secuencia guardad por el pi_old y vamos comparando los tokens seleccionados con la prob que arroja el modelo (o sea vamos usando el modelo pi_new para ver la prob que arroja para los tokens muestrados originalmente por el pi_old). Obviamente en la primera vuelta va a ser igual o casi igual que el pi_old porque es el mismo modelo, todavia no se actualizo. Usamos la formula de PPO, sumamos para cada token en la secuencia. -> **El advantage despues lo veo bien**\n",
    "- Lo mismo para cada ejemplo del batch. Y promediamos para cada ejemplo del batch.\n",
    "- Despues de cada minibatch, backprop y optimizamos!\n",
    "- Seguimos con los minibatches hasta terminar todos, hasta ahi 1 epoch\n",
    "- Hacemos los varios epochs lo mismo. OJO, el pi_new va acumulando los cambios PERO NO VA SAMPLEANDO.\n",
    "- DESPUES DE ESO, volvemos a samplear pero con el pi_new que reemplaza al pi_old.\n",
    "\n",
    "Si bien no sampleamos nuevas secuencias con los pi_new a medida que van cambiando, los vamos usando activamente para calcular la prob de cada token de la secuencia original no? O sea tienen un uso muy alto. \n",
    "A su vez,es como que los cambios en pi_new se van acumulando durante cada minibatch y eso por varios epochs no? O sea, acumulamos cambios y despues de varios epochs ahi lo usamos para generar una respuesta nueva?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec9955",
   "metadata": {},
   "source": [
    "**Importance sampling for Policy Gradient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de8b811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Policies ==\n",
      "theta_old=-0.700 -> pi_old(A)=0.332, pi_old(B)=0.668\n",
      "theta_new=+0.300 -> pi_new(A)=0.574, pi_new(B)=0.426\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "def softmax2(theta):\n",
    "    a = math.exp(theta)\n",
    "    b = 1.0\n",
    "    Z = a + b\n",
    "    return a/Z, b/Z  # (piA, piB)\n",
    "\n",
    "def grad_logpi_new(theta_new, action):\n",
    "    piA, piB = softmax2(theta_new)\n",
    "    if action == 0:   # A\n",
    "        return 1.0 - piA\n",
    "    else:             # B\n",
    "        return -piA\n",
    "\n",
    "def advantage(action):\n",
    "    return +1.0 if action == 0 else -1.0\n",
    "\n",
    "def sample_from_old(theta_old, N):\n",
    "    piA, piB = softmax2(theta_old)\n",
    "    samples = []\n",
    "    for _ in range(N):\n",
    "        samples.append(0 if random.random() < piA else 1)\n",
    "    return samples\n",
    "\n",
    "def exact_expectation_under_new(theta_new):\n",
    "    piA, piB = softmax2(theta_new)\n",
    "    gA = advantage(0) * grad_logpi_new(theta_new, 0)\n",
    "    gB = advantage(1) * grad_logpi_new(theta_new, 1)\n",
    "    return piA * gA + piB * gB\n",
    "\n",
    "theta_old = -0.7\n",
    "theta_new = +0.3\n",
    "N = 200\n",
    "\n",
    "piA_old, piB_old = softmax2(theta_old)\n",
    "piA_new, piB_new = softmax2(theta_new)\n",
    "\n",
    "print(\"== Policies ==\")\n",
    "print(f\"theta_old={theta_old:+.3f} -> pi_old(A)={piA_old:.3f}, pi_old(B)={piB_old:.3f}\")\n",
    "print(f\"theta_new={theta_new:+.3f} -> pi_new(A)={piA_new:.3f}, pi_new(B)={piB_new:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0dc11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample_from_old(theta_old, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ac92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, a in enumerate(samples):\n",
    "    pi_old_a = piA_old if a == 0 else piB_old\n",
    "    pi_new_a = piA_new if a == 0 else piB_new\n",
    "    r = pi_new_a / pi_old_a\n",
    "    g = advantage(a) * grad_logpi_new(theta_new, a)\n",
    "    rows.append({\n",
    "        \"i\": i,\n",
    "        \"action\": \"A\" if a == 0 else \"B\",\n",
    "        \"pi_old(a)\": pi_old_a,\n",
    "        \"pi_new(a)\": pi_new_a,\n",
    "        \"ratio r\": r,\n",
    "        \"A(a)\": advantage(a),\n",
    "        \"grad_logpi_new(a)\": grad_logpi_new(theta_new, a),\n",
    "        \"g = A * grad_logpi_new\": g,\n",
    "        \"r*g\": r * g,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "display_dataframe_to_user(\"IS demo ‚Äî per-sample contributions (scrollable)\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a134476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_mean = df[\"g = A * grad_logpi_new\"].mean()\n",
    "is_mean    = df[\"r*g\"].mean()\n",
    "true_mean  = exact_expectation_under_new(theta_new)\n",
    "\n",
    "countA = sum(1 for a in samples if a == 0)\n",
    "countB = N - countA\n",
    "\n",
    "print(\"\\n== Estimates of E_new[ A(a) * ‚àá log œÄ_new(a) ] ==\")\n",
    "print(f\"Ground truth (exact under pi_new): {true_mean:+.6f}\")\n",
    "print(f\"IS-corrected estimate (mean of r*g): {is_mean:+.6f}\")\n",
    "print(f\"WRONG (no IS; mean of g under old): {wrong_mean:+.6f}\")\n",
    "\n",
    "print(f\"\\nCounts from old: A={countA}  B={countB}  (expected under old: A~{N*piA_old:.1f}, B~{N*piB_old:.1f})\")\n",
    "print(f\"If sampled from new, expected counts: A~{N*piA_new:.1f}, B~{N*piB_new:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51422e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49c08451",
   "metadata": {},
   "source": [
    "**PPO simplificado** (solo parte de policy clipped. Sin KL ni entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "24035444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Hiperpar√°metros chiquitos\n",
    "vocab_size = 5\n",
    "T = 3\n",
    "lr = 0.5\n",
    "eps_clip = 0.2   # zona segura [0.8, 1.2]\n",
    "advantage = 0.4  # mismo A para todos los pasos (reward final - baseline)\n",
    "\n",
    "# Modelo lineal sin bias (como el tuyo)\n",
    "model = nn.Linear(T, vocab_size, bias=False)\n",
    "with torch.no_grad():\n",
    "    model.weight.copy_(0.1 * torch.randn(vocab_size, T))\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "# Estados one-hot (t = 0..T-1)\n",
    "I = torch.eye(T)\n",
    "\n",
    "# Elegimos una secuencia de tokens (simula lo que gener√≥ œÄ_old)\n",
    "chosen_tokens = [2, 1, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f91e6100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprob_old por paso: ['-1.7502', '-1.5884', '-1.6374']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 1) \"Foto\" de œÄ_old: guardamos logprob_old en esos (s_t, a_t)\n",
    "# ----------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    logprob_old = []\n",
    "    for t in range(T):\n",
    "        logits_t = model(I[t])          # (vocab,)\n",
    "        logp_t  = log_softmax(logits_t) # (vocab,)\n",
    "        tok = chosen_tokens[t]\n",
    "        logprob_old.append(float(logp_t[tok]))\n",
    "    logprob_old = torch.tensor(logprob_old, dtype=torch.float32)\n",
    "\n",
    "print(\"logprob_old por paso:\", [f\"{x:.4f}\" for x in logprob_old.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bac84040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprob_new BEFORE: ['-1.7502', '-1.5884', '-1.6374']\n",
      "P_new BEFORE      : ['0.1737', '0.2043', '0.1945']\n",
      "r_t BEFORE        : ['1.0000', '1.0000', '1.0000']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 2) BEFORE update: logprob_new y ratio (r_t ‚âà 1 al inicio)\n",
    "# ----------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    before = []\n",
    "    ratio_before = []\n",
    "    for t in range(T):\n",
    "        logits_t = model(I[t])\n",
    "        logp_t  = log_softmax(logits_t)\n",
    "        tok = chosen_tokens[t]\n",
    "        logp_new = logp_t[tok]\n",
    "        r_t = torch.exp(logp_new - logprob_old[t])\n",
    "        before.append((tok, float(logp_new), float(logp_t.exp()[tok])))\n",
    "        ratio_before.append(float(r_t))\n",
    "print(\"logprob_new BEFORE:\", [f\"{x[1]:.4f}\" for x in before])\n",
    "print(\"P_new BEFORE      :\", [f\"{x[2]:.4f}\" for x in before])\n",
    "print(\"r_t BEFORE        :\", [f\"{r:.4f}\" for r in ratio_before])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5c7daf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0: tok=2  logp_old=-1.7502  logp_new=-1.7502  r_t=1.0000  uncl=+0.4000  clp=+0.4000  picked=min(uncl,clp)\n",
      "t=1: tok=1  logp_old=-1.5884  logp_new=-1.5884  r_t=1.0000  uncl=+0.4000  clp=+0.4000  picked=min(uncl,clp)\n",
      "t=2: tok=4  logp_old=-1.6374  logp_new=-1.6374  r_t=1.0000  uncl=+0.4000  clp=+0.4000  picked=min(uncl,clp)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 3) Loss PPO (s√≥lo pol√≠tica, clipeada)\n",
    "#    L = - mean( min( r_t*A, clamp(r_t)*A ) )\n",
    "# ----------------------------------------------------------\n",
    "advantages = torch.full((T,), fill_value=advantage, dtype=torch.float32)\n",
    "\n",
    "total_unclipped = 0.0\n",
    "total_clipped = 0.0\n",
    "for t in range(T):\n",
    "    logits_t = model(I[t])\n",
    "    logp_t  = log_softmax(logits_t)\n",
    "    tok = chosen_tokens[t]\n",
    "    logp_new = logp_t[tok]\n",
    "    r_t = torch.exp(logp_new - logprob_old[t])\n",
    "    unclipped = r_t * advantages[t]\n",
    "    clipped_r = torch.clamp(r_t, 1.0 - eps_clip, 1.0 + eps_clip)\n",
    "    clipped = clipped_r * advantages[t]\n",
    "    total_unclipped = total_unclipped + unclipped\n",
    "    total_clipped   = total_clipped   + clipped\n",
    "    print(f\"t={t}: tok={tok}  logp_old={logprob_old[t]:+.4f}  logp_new={logp_new:+.4f}  r_t={float(r_t):.4f}  \"\n",
    "          f\"uncl={float(unclipped):+.4f}  clp={float(clipped):+.4f}  picked=min(uncl,clp)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5dffdb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Surrogate (mean): +0.400000  ->  PPO policy loss = -0.400000\n"
     ]
    }
   ],
   "source": [
    "# Promediamos el surrogate (min) y lo negamos para minimizar\n",
    "surrogate = torch.minimum(total_unclipped, total_clipped) / T\n",
    "loss = -surrogate\n",
    "print(f\"\\nSurrogate (mean): {float(surrogate):+.6f}  ->  PPO policy loss = {float(loss):+.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f4d42262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop + step\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62effa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-step chosen token probabilities (before -> after):\n",
      "  t=0: token=2   P_before=0.1737 -> P_after=0.1839   (‚Üë)\n",
      "  t=1: token=1   P_before=0.2043 -> P_after=0.2153   (‚Üë)\n",
      "  t=2: token=4   P_before=0.1945 -> P_after=0.2052   (‚Üë)\n",
      "r_t AFTER         : ['1.0583', '1.0540', '1.0552']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 4) AFTER update: ver c√≥mo cambiaron P_new de los tokens elegidos\n",
    "# ----------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    after = []\n",
    "    ratio_after = []\n",
    "    for t in range(T):\n",
    "        logits_t = model(I[t])\n",
    "        logp_t  = log_softmax(logits_t)\n",
    "        tok = chosen_tokens[t]\n",
    "        logp_new = logp_t[tok]\n",
    "        r_t = torch.exp(logp_new - logprob_old[t])\n",
    "        after.append((tok, float(logp_new), float(logp_t.exp()[tok])))\n",
    "        ratio_after.append(float(r_t))\n",
    "\n",
    "print(\"\\nPer-step chosen token probabilities (before -> after):\")\n",
    "for t, ((tok_b, logpb, pb), (tok_a, logpa, pa)) in enumerate(zip(before, after)):\n",
    "    assert tok_b == tok_a\n",
    "    arrow = \"‚Üë\" if pa > pb else (\"‚Üì\" if pa < pb else \"‚Üí\")\n",
    "    print(f\"  t={t}: token={tok_b}   P_before={pb:.4f} -> P_after={pa:.4f}   ({arrow})\")\n",
    "\n",
    "print(\"r_t AFTER         :\", [f\"{r:.4f}\" for r in ratio_after])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc1dea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519ba93c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55f875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6440781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874cb47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c303368c",
   "metadata": {},
   "source": [
    "**PPO**: \n",
    "\n",
    "usa un reward model aparte. \n",
    "\n",
    "\n",
    "**GRPO**:\n",
    "\n",
    "**DPO**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d096a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83985a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40c401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2de29207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0604,  0.0880, -0.0723,  0.0160,  0.2263],\n",
      "        [ 0.0811,  0.1048,  0.2866, -0.0025, -0.0918],\n",
      "        [-0.0045, -0.0045, -0.0566,  0.1074, -0.0225]], grad_fn=<MmBackward0>)\n",
      "tensor([[-1.6176, -1.5900, -1.7502, -1.6619, -1.4517],\n",
      "        [-1.6121, -1.5884, -1.4065, -1.6957, -1.7849],\n",
      "        [-1.6194, -1.6193, -1.6714, -1.5075, -1.6374]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0.1984, 0.2039, 0.1737, 0.1898, 0.2342],\n",
      "        [0.1995, 0.2043, 0.2450, 0.1835, 0.1678],\n",
      "        [0.1980, 0.1980, 0.1880, 0.2215, 0.1945]], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "vocab_size=5\n",
    "T=3\n",
    "lr=0.5\n",
    "\n",
    "model = nn.Linear(T, vocab_size, bias=False)\n",
    "with torch.no_grad():\n",
    "    model.weight.copy_(0.1 * torch.randn(vocab_size, T))\n",
    "log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "I = torch.eye(T)  # shape (T, T)\n",
    "\n",
    "logits_t = model(I)          # (vocab,)\n",
    "print(logits_t)\n",
    "\n",
    "logp_t  = log_softmax(logits_t) # (vocab,)\n",
    "print(logp_t)\n",
    "\n",
    "probs = logp_t.exp()\n",
    "print(probs)\n",
    "\n",
    "probs.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb90c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
