{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c52021",
   "metadata": {},
   "source": [
    "# Intro RL para LLMs\n",
    "\n",
    "- Agent/policy model: Estrategia para tomar decisiones. Es el LLM\n",
    "- Environment: provee feedback al agente. Puede ser muchas cosas aca\n",
    "- Action: Pueden ser PALABRAS, RESPUESTA ENTERA, OUTCOME FINAL, etc\n",
    "- Reward: Feedback que le da el env al agent. Numero\n",
    "\n",
    "---\n",
    "\n",
    "**RLHF**\n",
    "\n",
    "- Preferencias humanas: a partir de varias respuestas, las ranqueamos\n",
    "- Reward model: construimos un reward model basado en esas preferencias. Ahora tenemos un modelo que, dado una respuesta, estima el puntaje que le darian humanos\n",
    "- Fine-tune el LLM con RL: Ahora el reward model nos da el feedback. Entonces:\n",
    "    - El agente genera respuestas\n",
    "    - El reward model las scorea\n",
    "    - Ajustamos los weights para que tienda a producir outputs que sean bien puntuados\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**RL vs SL**\n",
    "\n",
    "Principalmente cambia la fuente de la señal (reward). Qué optimizas realmente\n",
    "- SL: Minimizar divergencia respecto a la distribución objetivo (imitar). Tiene un reward **denso** porque sabes la respuesta target para cada token.\n",
    "- RL: Maximizar retorno esperado bajo dinámicas del entorno (elegir). La señal es escasa y esta diferida en el tiempo. Se requiere **exploracion** y credito temporal. \n",
    "\n",
    "Por eso se dice que SFT memorizes y RL generalizes https://arxiv.org/abs/2501.17161\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89482550",
   "metadata": {},
   "source": [
    "# Policy optimization (gradient-based)\n",
    "\n",
    "Tecnicas para optimizar el LLM / policy model para que mejore las predicciones.\n",
    "\n",
    "**Intro**:\n",
    "\n",
    "En SL tenemos una funcion de perdida (loss) y lo que queremos es minimizar esa loss. Entonces dado un batch, calculamos la loss (un numero final) basado en muchos errores (1 o varios para cada elemento del batch), los promediamos y nos queda ese numero final. Ahi, buscamos el gradiente (el conjunto de derivadas parciales de cada uno de los parametros aprendibles) que hagan que mas baje la loss en ese punto. Y actualizamos los valores de los parametros en la direccion y fuerza de esas derivadas y agregandole otros empujones.\n",
    "\n",
    "En RL, lo que tratamos de hacer es subir o bajar la probabilidad de los tokens segun si aparecen en la secuencia que genero un buen o mal resultado (bien o mal comparado con un baseline).\n",
    "\n",
    "\n",
    "**Pasos general (on-policy)**:\n",
    "- Se hacen rollouts (el modelo genera respuestas completas)\n",
    "- Se evalua cada respuesta y se obtiene score o reward\n",
    "- El score se convierte en ventaje (advantage): cuanto mejor o peor esta respuesta fue a lo esperado o tipico (por ej del grupo o de algo de referencia).\n",
    "    - Si tengo solo reward final, el advantage se aplica a todos los tokens de la secuencia de la misma manera (credito global)\n",
    "    - Si tengo verificadores por paso, se reparten ventajas distintas por token\n",
    "- Distribuyo la ventaja para cada token:\n",
    "    - Si es positiva, subo su log-prob (empujo logits para arriba).\n",
    "    - Si es negativa, la bajo.\n",
    "    - Indirectamnete afecto a las otras (porque en probs la suma = 1)\n",
    "- Regularizo para no irme tan lejos de una distribucion anterior (basada en una politica de referencia) para evitar colapso.\n",
    "\n",
    "**Caso general**\n",
    "- Nosotros tenemos una red y para un estado (secuenci), produce logits (next token / action)\n",
    "- Los pasamos por softmax y tenemos probs y de esos tomamos el log, por lo que trabajamos con log probs. \n",
    "- Para el token elegido en un paso t, vemos el log prob (solo tenemos un log prob en ese paso)\n",
    "- Si tomamos el gradiente de ese log prob, estamos viendo un conjunto de derivadas parciales del logprob con respecto a todos los parametros del modelo. Esto indica la direccion de MAXIMA SUBIDA -> o sea, si tomamos un paso en esa direccion, aumentamos la logprob de ese token para la prox.\n",
    "- Si hacemos lo mismo para cada token de la secuencia de pasos generados, y sumamos los gradientes (sumamos las derivadas parciales (o sea, para un parametro miramos las derivadas parciales por cada token en la secuencia generada y los sumamos)), nos queda un gradiente unico que apunta a maximizar todos los tokens en la secuencia generada. \n",
    "- Despues vemos el reward para esa secuencia y lo comparamos con el baseline (R-b). Si el baseline es 0.5 y nuestro reward es 0.2, el advantage da -0.3. Eso lo usamos para multiplicar, lo que nos cambia la direccion del gradiente si el advantage es negativo y tambien nos da una magnitud de cambio (escala el tamaño del paso junto con el lr). O sea que si es negativo, en vez de apuntar a aumentar la prob, apunta a disminuir la prob de los tokens.\n",
    "- Para no favorecer secuencias largas, a veces se promedia la suma de logprobs y se suele agregar entropia (para mantener diversidad) y KL a un modelo de referencia para regularizar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a044d675",
   "metadata": {},
   "source": [
    "## REINFORCE\n",
    "\n",
    "$$\\text L_{(REINFORCE)} = - (R - b) \\sum_t \\log \\pi_\\theta(a_t \\mid s_t)$$\n",
    "\n",
    "$$\\text L_{(REINFORCE)} = - \\sum_t (R - b) \\log \\pi_\\theta(a_t \\mid s_t)$$\n",
    "\n",
    "El gradiente para la secuencia entera seria:\n",
    "\n",
    "$$\\Delta \\theta \\propto (R - b) \\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ff1132a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0604,  0.0811, -0.0045],\n",
       "        [ 0.0880,  0.1048, -0.0045],\n",
       "        [-0.0723,  0.2866, -0.0566],\n",
       "        [ 0.0160, -0.0025,  0.1074],\n",
       "        [ 0.2263, -0.0918, -0.0225]], requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "vocab_size=5\n",
    "T=3\n",
    "lr=0.5\n",
    "\n",
    "# Model: linear map from one-hot(state) to vocab logits (bias-free for clarity).\n",
    "model = nn.Linear(T, vocab_size, bias=False)\n",
    "# Initialize weights small & reproducible\n",
    "with torch.no_grad():\n",
    "    model.weight.copy_(0.1 * torch.randn(vocab_size, T))\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "142af8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inputs: one-hot vectors for each time step 0..T-1\n",
    "I = torch.eye(T)  # shape (T, T)\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8794e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, -1.750232458114624, 0.17373354732990265),\n",
       " (1, -1.5883560180664062, 0.20426113903522491),\n",
       " (4, -1.6373724937438965, 0.19449038803577423)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CASO 1 (GOOD)\n",
    "chosen_tokens = [2, 1, 4]\n",
    "advantage = 0.4\n",
    "\n",
    "# --- Forward BEFORE update: record chosen tokens' log-probs and probs ---\n",
    "with torch.no_grad():\n",
    "    before = []\n",
    "    for t in range(T):\n",
    "        logits_t = model(I[t])          # (vocab,)\n",
    "        logp_t  = log_softmax(logits_t) # (vocab,)\n",
    "        tok = chosen_tokens[t]\n",
    "        before.append((tok, float(logp_t[tok]), float(logp_t.exp()[tok])))\n",
    "before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca91b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0: token 2 logprob=-1.750232458114624 total_logprob=-1.750232458114624\n",
      "t=1: token 1 logprob=-1.5883560180664062 total_logprob=-3.3385884761810303\n",
      "t=2: token 4 logprob=-1.6373724937438965 total_logprob=-4.975960731506348\n",
      "-advantage=-0.4\n",
      "-advantage * total_logprob = -0.4 - -4.975960731506348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.9904, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Compute REINFORCE loss:  L = -A * sum_t log pi(a_t | s_t) ---\n",
    "total_logprob = 0.0\n",
    "for t in range(T):\n",
    "    # sumamos todos los logprobs de los tokens elegidos\n",
    "    logits_t = model(I[t])          # (vocab,)\n",
    "    logp_t  = log_softmax(logits_t) # (vocab,)\n",
    "    tok = chosen_tokens[t]\n",
    "    total_logprob = total_logprob + logp_t[tok]\n",
    "    print(f't={t}: token {tok} logprob={logp_t[tok]} total_logprob={total_logprob}')\n",
    "print(f'-advantage={-advantage}\\n-advantage * total_logprob = {-advantage} - {total_logprob}')\n",
    "loss = - advantage * total_logprob\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c798d9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0207,  0.0412, -0.0441],\n",
       "        [ 0.0472,  0.2640, -0.0441],\n",
       "        [ 0.0930,  0.2376, -0.0941],\n",
       "        [-0.0219, -0.0392,  0.0631],\n",
       "        [ 0.1794, -0.1253,  0.1386]], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Backprop + step\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "opt.step()\n",
    "\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2c443e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-step chosen token probabilities (before -> after):\n",
      "  t=0: token=2   P_before=0.1737 -> P_after=0.2055   (↑)\n",
      "  t=1: token=1   P_before=0.2043 -> P_after=0.2386   (↑)\n",
      "  t=2: token=4   P_before=0.1945 -> P_after=0.2280   (↑)\n"
     ]
    }
   ],
   "source": [
    "# --- Forward AFTER update: record chosen tokens' log-probs and probs ---\n",
    "with torch.no_grad():\n",
    "    after = []\n",
    "    for t in range(T):\n",
    "        logits_t = model(I[t])          # (vocab,)\n",
    "        logp_t  = log_softmax(logits_t) # (vocab,)\n",
    "        tok = chosen_tokens[t]\n",
    "        after.append((tok, float(logp_t[tok]), float(logp_t.exp()[tok])))\n",
    "\n",
    "print(\"Per-step chosen token probabilities (before -> after):\")\n",
    "for t, ((tok_b, logpb, pb), (tok_a, logpa, pa)) in enumerate(zip(before, after)):\n",
    "    assert tok_b == tok_a\n",
    "    arrow = \"↑\" if pa > pb else (\"↓\" if pa < pb else \"→\")\n",
    "    print(f\"  t={t}: token={tok_b}   P_before={pb:.4f} -> P_after={pa:.4f}   ({arrow})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa8513b",
   "metadata": {},
   "source": [
    "## PPO\n",
    "\n",
    "Limita cuanto se puede cambiar la distribucion para que no hayan pasos demasiado grandes y colapse.\n",
    "\n",
    "Hay un ratio por token\n",
    "\n",
    "$$r_t = \\frac{\\pi_{\\text{nueva}}(a_t \\mid s_t)}{\\pi_{\\text{vieja}}(a_t \\mid s_t)}$$\n",
    "\n",
    "Que te dice cuanto cambiaste la prob para un token. Si es 1.3, subiste 30% la probabilidad para ese token. Si es 0.7 bajaste 30% la prob para ese token.\n",
    "\n",
    "- **Clipping**: Si el paso te lleva a un r_t demasiado grande > 1 + eps, entonces recorto el beneficio\n",
    "- **KL divergence a un modelo referencia**: Penaliza alejarte del modelo base de referencia (el original) en promedio.\n",
    "- **Ventaja por paso** (opcional): en vez de tener un unico A global, lo hacemos por paso, con V(s_t) para asignar mejor el credito y premiar a los tokens clave. Si no esta, se puede usar como baseline la media del batch.\n",
    "- **Entropia**: bonus que mantiene la exploracion y evita colapso de entropia (que se vuelva modo unico)\n",
    "\n",
    "**PPO Loss**:\n",
    "\n",
    "$$L = \\mathbb{E}\\left[ \\min\\left(r_t A_t, \\; \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A_t \\right) \\right] - \\beta \\, \\mathrm{KL}(\\pi_{\\text{new}} \\| \\pi_{\\text{ref}}) + \\alpha \\, \\text{Entropía}$$\n",
    "\n",
    "Tres grandes partes:\n",
    "\n",
    "- Politica -> $\\mathbb{E}\\left[ \\min\\left(r_t A_t, \\; \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A_t \\right) \\right]$\n",
    "- KL -> $- \\beta \\, \\mathrm{KL}(\\pi_{\\text{new}} \\| \\pi_{\\text{ref}})$\n",
    "- Entropia -> $\\alpha \\, \\text{Entropía}$\n",
    "\n",
    "\n",
    "Politica\n",
    "\n",
    "$r_t = \\frac{\\pi_{\\text{new}}(a_t \\mid s_t)}{\\pi_{\\text{old}}(a_t \\mid s_t)} = \\exp\\left( \\log \\pi_{\\text{new}}(a_t \\mid s_t) - \\log \\pi_{\\text{old}}(a_t \\mid s_t) \\right)$\n",
    "\n",
    "- $\\log \\pi_{\\text{new}}(a_t \\mid s_t)$ -> se hace backprop\n",
    "- $\\log \\pi_{\\text{old}}(a_t \\mid s_t)$ -> constante\n",
    "\n",
    "Como en REINFORCE, aca tambien aparece el **logprob $\\log \\pi(a_t \\mid s_t)$. Por aca fluye el gradiente**.\n",
    "\n",
    "PPO es REINFORCE con:\n",
    "- Un peso r_t que te dice cuanto ya cambiaste la prob\n",
    "- Un clip que apaga el grad si te pasaste. Si el clip se activa (te pasaste), el término se vuelve constante → gradiente 0 (no empuja más ese token en este minibatch). El clip define una ZONA segura: por ej [0.8, 1.2], entonces el ratio siempre deberia estar ahi adentro. Si r_t sale de esa zona, apagamos el grad.\n",
    "\n",
    "\n",
    "**Gradiente**: Cuando usamos r_t, tenemos la propiedad que el gradiente del ratio es el grad del logprob_new multiplicado por el valor posta del ratio\n",
    "\n",
    "- $\\nabla r = r \\cdot \\nabla \\log \\pi_{\\text{new}}$\n",
    "\n",
    "Entonces el gradiente (solo de policy y sin usar CLIP), simplificado, queda:\n",
    "\n",
    "$$\\Delta \\theta \\propto \\sum_t A \\cdot r_{t} \\cdot \\nabla \\log \\pi_{\\text{new}}$$ \n",
    "\n",
    "Entonces, cuando r_t = 1 (al principio cuando las dos policies son iguales), entonces el gradiente es igual a REINFORCE!\n",
    "\n",
    "**El ratio solo pondera** y permite clippear, pero en cuanto a grad, es lo mismo que reinforce casi.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**FLUJO**\n",
    "\n",
    "Los policies new NO SAMPLEAN. Lo unico que samplea es el OLD (se va actualizando cada tanto).\n",
    "\n",
    "- Tenemos un modelo pi_old. Tenemos prompts. Creamos minibatches (por ejemplo 4 prompts en cada minibatch) y tenemos 3 minibatches (12 ejemplos en total). \n",
    "- Sampleamos la respuesta usando pi_old y guardamos toda la info (probs de los tokens para CADA step, reward, etc) para cada prompt en todos los minibatches.\n",
    "- Ahora empieza la primera corrida de varios EPOCH (1 epoch = 1 pasada por todos los minibatches). Esto se va a hacer SIN HACER NINGUN SAMPLEO A MODELOS. Solo con los datos que estan.\n",
    "- Agarramos el primer minibatch de 4 ejemplos. Para cada ejemplo ya tenemos la secuencia/respuesta originada por el pi_old. Agarramos el primer ejemplo y vamos token por token de la secuencia guardad por el pi_old y vamos comparando los tokens seleccionados con la prob que arroja el modelo (o sea vamos usando el modelo pi_new para ver la prob que arroja para los tokens muestrados originalmente por el pi_old). Obviamente en la primera vuelta va a ser igual o casi igual que el pi_old porque es el mismo modelo, todavia no se actualizo. Usamos la formula de PPO, sumamos para cada token en la secuencia. -> **El advantage despues lo veo bien**\n",
    "- Lo mismo para cada ejemplo del batch. Y promediamos para cada ejemplo del batch.\n",
    "- Despues de cada minibatch, backprop y optimizamos!\n",
    "- Seguimos con los minibatches hasta terminar todos, hasta ahi 1 epoch\n",
    "- Hacemos los varios epochs lo mismo. OJO, el pi_new va acumulando los cambios PERO NO VA SAMPLEANDO.\n",
    "- DESPUES DE ESO, volvemos a samplear pero con el pi_new que reemplaza al pi_old."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c08451",
   "metadata": {},
   "source": [
    "**PPO simplificado** (solo parte de policy clipped. Sin KL ni entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "24035444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Hiperparámetros chiquitos\n",
    "vocab_size = 5\n",
    "T = 3\n",
    "lr = 0.5\n",
    "eps_clip = 0.2   # zona segura [0.8, 1.2]\n",
    "advantage = 0.4  # mismo A para todos los pasos (reward final - baseline)\n",
    "\n",
    "# Modelo lineal sin bias (como el tuyo)\n",
    "model = nn.Linear(T, vocab_size, bias=False)\n",
    "with torch.no_grad():\n",
    "    model.weight.copy_(0.1 * torch.randn(vocab_size, T))\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "# Estados one-hot (t = 0..T-1)\n",
    "I = torch.eye(T)\n",
    "\n",
    "# Elegimos una secuencia de tokens (simula lo que generó π_old)\n",
    "chosen_tokens = [2, 1, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f91e6100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprob_old por paso: ['-1.7502', '-1.5884', '-1.6374']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 1) \"Foto\" de π_old: guardamos logprob_old en esos (s_t, a_t)\n",
    "# ----------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    logprob_old = []\n",
    "    for t in range(T):\n",
    "        logits_t = model(I[t])          # (vocab,)\n",
    "        logp_t  = log_softmax(logits_t) # (vocab,)\n",
    "        tok = chosen_tokens[t]\n",
    "        logprob_old.append(float(logp_t[tok]))\n",
    "    logprob_old = torch.tensor(logprob_old, dtype=torch.float32)\n",
    "\n",
    "print(\"logprob_old por paso:\", [f\"{x:.4f}\" for x in logprob_old.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bac84040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprob_new BEFORE: ['-1.7502', '-1.5884', '-1.6374']\n",
      "P_new BEFORE      : ['0.1737', '0.2043', '0.1945']\n",
      "r_t BEFORE        : ['1.0000', '1.0000', '1.0000']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 2) BEFORE update: logprob_new y ratio (r_t ≈ 1 al inicio)\n",
    "# ----------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    before = []\n",
    "    ratio_before = []\n",
    "    for t in range(T):\n",
    "        logits_t = model(I[t])\n",
    "        logp_t  = log_softmax(logits_t)\n",
    "        tok = chosen_tokens[t]\n",
    "        logp_new = logp_t[tok]\n",
    "        r_t = torch.exp(logp_new - logprob_old[t])\n",
    "        before.append((tok, float(logp_new), float(logp_t.exp()[tok])))\n",
    "        ratio_before.append(float(r_t))\n",
    "print(\"logprob_new BEFORE:\", [f\"{x[1]:.4f}\" for x in before])\n",
    "print(\"P_new BEFORE      :\", [f\"{x[2]:.4f}\" for x in before])\n",
    "print(\"r_t BEFORE        :\", [f\"{r:.4f}\" for r in ratio_before])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5c7daf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0: tok=2  logp_old=-1.7502  logp_new=-1.7502  r_t=1.0000  uncl=+0.4000  clp=+0.4000  picked=min(uncl,clp)\n",
      "t=1: tok=1  logp_old=-1.5884  logp_new=-1.5884  r_t=1.0000  uncl=+0.4000  clp=+0.4000  picked=min(uncl,clp)\n",
      "t=2: tok=4  logp_old=-1.6374  logp_new=-1.6374  r_t=1.0000  uncl=+0.4000  clp=+0.4000  picked=min(uncl,clp)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 3) Loss PPO (sólo política, clipeada)\n",
    "#    L = - mean( min( r_t*A, clamp(r_t)*A ) )\n",
    "# ----------------------------------------------------------\n",
    "advantages = torch.full((T,), fill_value=advantage, dtype=torch.float32)\n",
    "\n",
    "total_unclipped = 0.0\n",
    "total_clipped = 0.0\n",
    "for t in range(T):\n",
    "    logits_t = model(I[t])\n",
    "    logp_t  = log_softmax(logits_t)\n",
    "    tok = chosen_tokens[t]\n",
    "    logp_new = logp_t[tok]\n",
    "    r_t = torch.exp(logp_new - logprob_old[t])\n",
    "    unclipped = r_t * advantages[t]\n",
    "    clipped_r = torch.clamp(r_t, 1.0 - eps_clip, 1.0 + eps_clip)\n",
    "    clipped = clipped_r * advantages[t]\n",
    "    total_unclipped = total_unclipped + unclipped\n",
    "    total_clipped   = total_clipped   + clipped\n",
    "    print(f\"t={t}: tok={tok}  logp_old={logprob_old[t]:+.4f}  logp_new={logp_new:+.4f}  r_t={float(r_t):.4f}  \"\n",
    "          f\"uncl={float(unclipped):+.4f}  clp={float(clipped):+.4f}  picked=min(uncl,clp)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5dffdb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Surrogate (mean): +0.400000  ->  PPO policy loss = -0.400000\n"
     ]
    }
   ],
   "source": [
    "# Promediamos el surrogate (min) y lo negamos para minimizar\n",
    "surrogate = torch.minimum(total_unclipped, total_clipped) / T\n",
    "loss = -surrogate\n",
    "print(f\"\\nSurrogate (mean): {float(surrogate):+.6f}  ->  PPO policy loss = {float(loss):+.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f4d42262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop + step\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62effa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-step chosen token probabilities (before -> after):\n",
      "  t=0: token=2   P_before=0.1737 -> P_after=0.1839   (↑)\n",
      "  t=1: token=1   P_before=0.2043 -> P_after=0.2153   (↑)\n",
      "  t=2: token=4   P_before=0.1945 -> P_after=0.2052   (↑)\n",
      "r_t AFTER         : ['1.0583', '1.0540', '1.0552']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 4) AFTER update: ver cómo cambiaron P_new de los tokens elegidos\n",
    "# ----------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    after = []\n",
    "    ratio_after = []\n",
    "    for t in range(T):\n",
    "        logits_t = model(I[t])\n",
    "        logp_t  = log_softmax(logits_t)\n",
    "        tok = chosen_tokens[t]\n",
    "        logp_new = logp_t[tok]\n",
    "        r_t = torch.exp(logp_new - logprob_old[t])\n",
    "        after.append((tok, float(logp_new), float(logp_t.exp()[tok])))\n",
    "        ratio_after.append(float(r_t))\n",
    "\n",
    "print(\"\\nPer-step chosen token probabilities (before -> after):\")\n",
    "for t, ((tok_b, logpb, pb), (tok_a, logpa, pa)) in enumerate(zip(before, after)):\n",
    "    assert tok_b == tok_a\n",
    "    arrow = \"↑\" if pa > pb else (\"↓\" if pa < pb else \"→\")\n",
    "    print(f\"  t={t}: token={tok_b}   P_before={pb:.4f} -> P_after={pa:.4f}   ({arrow})\")\n",
    "\n",
    "print(\"r_t AFTER         :\", [f\"{r:.4f}\" for r in ratio_after])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc1dea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519ba93c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55f875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6440781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874cb47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c303368c",
   "metadata": {},
   "source": [
    "**PPO**: \n",
    "\n",
    "usa un reward model aparte. \n",
    "\n",
    "\n",
    "**GRPO**:\n",
    "\n",
    "**DPO**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d096a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83985a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40c401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2de29207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0604,  0.0880, -0.0723,  0.0160,  0.2263],\n",
      "        [ 0.0811,  0.1048,  0.2866, -0.0025, -0.0918],\n",
      "        [-0.0045, -0.0045, -0.0566,  0.1074, -0.0225]], grad_fn=<MmBackward0>)\n",
      "tensor([[-1.6176, -1.5900, -1.7502, -1.6619, -1.4517],\n",
      "        [-1.6121, -1.5884, -1.4065, -1.6957, -1.7849],\n",
      "        [-1.6194, -1.6193, -1.6714, -1.5075, -1.6374]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor([[0.1984, 0.2039, 0.1737, 0.1898, 0.2342],\n",
      "        [0.1995, 0.2043, 0.2450, 0.1835, 0.1678],\n",
      "        [0.1980, 0.1980, 0.1880, 0.2215, 0.1945]], grad_fn=<ExpBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "vocab_size=5\n",
    "T=3\n",
    "lr=0.5\n",
    "\n",
    "model = nn.Linear(T, vocab_size, bias=False)\n",
    "with torch.no_grad():\n",
    "    model.weight.copy_(0.1 * torch.randn(vocab_size, T))\n",
    "log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "I = torch.eye(T)  # shape (T, T)\n",
    "\n",
    "logits_t = model(I)          # (vocab,)\n",
    "print(logits_t)\n",
    "\n",
    "logp_t  = log_softmax(logits_t) # (vocab,)\n",
    "print(logp_t)\n",
    "\n",
    "probs = logp_t.exp()\n",
    "print(probs)\n",
    "\n",
    "probs.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb90c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
