{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c52021",
   "metadata": {},
   "source": [
    "# Intro RL para LLMs\n",
    "\n",
    "- Agent/policy model: Estrategia para tomar decisiones. Es el LLM\n",
    "- Environment: provee feedback al agente. Puede ser muchas cosas aca\n",
    "- Action: Pueden ser PALABRAS, RESPUESTA ENTERA, OUTCOME FINAL, etc\n",
    "- Reward: Feedback que le da el env al agent. Numero\n",
    "\n",
    "---\n",
    "\n",
    "**RLHF**\n",
    "\n",
    "- Preferencias humanas: a partir de varias respuestas, las ranqueamos\n",
    "- Reward model: construimos un reward model basado en esas preferencias. Ahora tenemos un modelo que, dado una respuesta, estima el puntaje que le darian humanos\n",
    "- Fine-tune el LLM con RL: Ahora el reward model nos da el feedback. Entonces:\n",
    "    - El agente genera respuestas\n",
    "    - El reward model las scorea\n",
    "    - Ajustamos los weights para que tienda a producir outputs que sean bien puntuados\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**RL vs SL**\n",
    "\n",
    "Principalmente cambia la fuente de la se√±al (reward). Qu√© optimizas realmente\n",
    "- SL: Minimizar divergencia respecto a la distribuci√≥n objetivo (imitar). Tiene un reward **denso** porque sabes la respuesta target para cada token.\n",
    "- RL: Maximizar retorno esperado bajo din√°micas del entorno (elegir). La se√±al es escasa y esta diferida en el tiempo. Se requiere **exploracion** y credito temporal. \n",
    "\n",
    "Por eso se dice que SFT memorizes y RL generalizes https://arxiv.org/abs/2501.17161\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89482550",
   "metadata": {},
   "source": [
    "# Policy optimization (gradient-based)\n",
    "\n",
    "Tecnicas para optimizar el LLM / policy model para que mejore las predicciones.\n",
    "\n",
    "**Intro**:\n",
    "\n",
    "En SL tenemos una funcion de perdida (loss) y lo que queremos es minimizar esa loss. Entonces dado un batch, calculamos la loss (un numero final) basado en muchos errores (1 o varios para cada elemento del batch), los promediamos y nos queda ese numero final. Ahi, buscamos el gradiente (el conjunto de derivadas parciales de cada uno de los parametros aprendibles) que hagan que mas baje la loss en ese punto. Y actualizamos los valores de los parametros en la direccion y fuerza de esas derivadas y agregandole otros empujones.\n",
    "\n",
    "En RL, lo que tratamos de hacer es subir o bajar la probabilidad de los tokens segun si aparecen en la secuencia que genero un buen o mal resultado (bien o mal comparado con un baseline).\n",
    "\n",
    "\n",
    "**Pasos general (on-policy)**:\n",
    "- Se hacen rollouts (el modelo genera respuestas completas)\n",
    "- Se evalua cada respuesta y se obtiene score o reward\n",
    "- El score se convierte en ventaje (advantage): cuanto mejor o peor esta respuesta fue a lo esperado o tipico (por ej del grupo o de algo de referencia).\n",
    "    - Si tengo solo reward final, el advantage se aplica a todos los tokens de la secuencia de la misma manera (credito global)\n",
    "    - Si tengo verificadores por paso, se reparten ventajas distintas por token\n",
    "- Distribuyo la ventaja para cada token:\n",
    "    - Si es positiva, subo su log-prob (empujo logits para arriba).\n",
    "    - Si es negativa, la bajo.\n",
    "    - Indirectamnete afecto a las otras (porque en probs la suma = 1)\n",
    "- Regularizo para no irme tan lejos de una distribucion anterior (basada en una politica de referencia) para evitar colapso.\n",
    "\n",
    "**Caso general**\n",
    "- Nosotros tenemos una red y para un estado (secuenci), produce logits (next token / action)\n",
    "- Los pasamos por softmax y tenemos probs y de esos tomamos el log, por lo que trabajamos con log probs. \n",
    "- Para el token elegido en un paso t, vemos el log prob (solo tenemos un log prob en ese paso)\n",
    "- Si tomamos el gradiente de ese log prob, estamos viendo un conjunto de derivadas parciales del logprob con respecto a todos los parametros del modelo. Esto indica la direccion de MAXIMA SUBIDA -> o sea, si tomamos un paso en esa direccion, aumentamos la logprob de ese token para la prox.\n",
    "- Si hacemos lo mismo para cada token de la secuencia de pasos generados, y sumamos los gradientes (sumamos las derivadas parciales (o sea, para un parametro miramos las derivadas parciales por cada token en la secuencia generada y los sumamos)), nos queda un gradiente unico que apunta a maximizar todos los tokens en la secuencia generada. \n",
    "- Despues vemos el reward para esa secuencia y lo comparamos con el baseline (R-b). Si el baseline es 0.5 y nuestro reward es 0.2, el advantage da -0.3. Eso lo usamos para multiplicar, lo que nos cambia la direccion del gradiente si el advantage es negativo y tambien nos da una magnitud de cambio (escala el tama√±o del paso junto con el lr). O sea que si es negativo, en vez de apuntar a aumentar la prob, apunta a disminuir la prob de los tokens.\n",
    "- Para no favorecer secuencias largas, a veces se promedia la suma de logprobs y se suele agregar entropia (para mantener diversidad) y KL a un modelo de referencia para regularizar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a044d675",
   "metadata": {},
   "source": [
    "## REINFORCE\n",
    "\n",
    "Loss function: \n",
    "\n",
    "$$\\text L_\\theta = - (R - b) \\sum_t \\log \\pi_\\theta(a_t \\mid s_t)$$\n",
    "\n",
    "- Partimos de Maximum Likelihood: queremos que el modelo asigne alta probabilidad a lo que efectivamente ocurri√≥ (los datos reales).\n",
    "- Para secuencias, eso se traduce en un producto de probabilidades de cada token de la secuencia.\n",
    "- Como trabajar con productos es num√©ricamente inestable (tienden a 0), usamos logaritmo: eso convierte el producto en suma.\n",
    "- Advantage ùê¥=ùëÖ‚àíùëè: En RL, no todas las muestras valen lo mismo: algunas tuvieron mejor reward. Por eso ponderamos la suma con el advantage. El Advantage puede ser por paso (cuando haya reward por paso u otra tecnica) o general para la secuencia (para una secuencia particular, con respecto a otras (baseline)). Se puede poner dentro de la suma.\n",
    "\n",
    "$$\\text L_\\theta = - \\sum_t A_t \\cdot \\log \\pi_\\theta(a_t \\mid s_t)$$\n",
    "\n",
    "- La loss depende de Œ∏. Como regla de gradient descent tenemos: $\\theta \\leftarrow \\theta - a \\cdot \\nabla_\\theta L$.\n",
    "- Queremos derivar la loss respecto a Œ∏, modificando los pesos (que es lo unico que podemos mover) para que la loss sea mas chica.\n",
    "- Tenemos la politica (que es el LLM), que esto lo podemos mover, porque depende de Œ∏. Entonces lo queremos derivar: $\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)$\n",
    "- El gradiente de una funcion siempre nos dice el steepest ascent de la funcion con respecto a un parametro.\n",
    "- Si tenemos una Loss, vamos a querer derivarla respecto a los parametros del modelo. Entonces buscamos en la formula aquello que dependa de los parametros del modelo (pi)\n",
    "\n",
    "Entonces, el gradiente para la secuencia entera seria:\n",
    "\n",
    "$$\\nabla_\\theta L_\\theta = - \\sum_t A_t \\cdot \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)$$\n",
    "\n",
    "Este es el vector del gradiente, que apunta a donde mas crece L.\n",
    "\n",
    "Y en cuanto a gradient descent tenemos que, en cada optimizacion, theta cambia asi:\n",
    "\n",
    "$$\\Delta \\theta = - a \\cdot \\nabla_\\theta L(\\theta)$$\n",
    "\n",
    "O sea: \n",
    "\n",
    "$$\\theta \\leftarrow \\theta - a \\cdot \\nabla_\\theta L(\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ff1132a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0604,  0.0811, -0.0045],\n",
       "        [ 0.0880,  0.1048, -0.0045],\n",
       "        [-0.0723,  0.2866, -0.0566],\n",
       "        [ 0.0160, -0.0025,  0.1074],\n",
       "        [ 0.2263, -0.0918, -0.0225]], requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "vocab_size=5\n",
    "T=3\n",
    "lr=0.5\n",
    "\n",
    "# Model: linear map from one-hot(state) to vocab logits (bias-free for clarity).\n",
    "model = nn.Linear(T, vocab_size, bias=False)\n",
    "# Initialize weights small & reproducible\n",
    "with torch.no_grad():\n",
    "    model.weight.copy_(0.1 * torch.randn(vocab_size, T))\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "142af8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inputs: one-hot vectors for each time step 0..T-1\n",
    "I = torch.eye(T)  # shape (T, T)\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8794e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, -1.750232458114624, 0.17373354732990265),\n",
       " (1, -1.5883560180664062, 0.20426113903522491),\n",
       " (4, -1.6373724937438965, 0.19449038803577423)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CASO 1 (GOOD)\n",
    "chosen_tokens = [2, 1, 4]\n",
    "advantage = 0.4\n",
    "\n",
    "# --- Forward BEFORE update: record chosen tokens' log-probs and probs ---\n",
    "with torch.no_grad():\n",
    "    before = []\n",
    "    for t in range(T):\n",
    "        logits_t = model(I[t])          # (vocab,)\n",
    "        logp_t  = log_softmax(logits_t) # (vocab,)\n",
    "        tok = chosen_tokens[t]\n",
    "        before.append((tok, float(logp_t[tok]), float(logp_t.exp()[tok])))\n",
    "before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca91b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0: token 2 logprob=-1.750232458114624 total_logprob=-1.750232458114624\n",
      "t=1: token 1 logprob=-1.5883560180664062 total_logprob=-3.3385884761810303\n",
      "t=2: token 4 logprob=-1.6373724937438965 total_logprob=-4.975960731506348\n",
      "-advantage=-0.4\n",
      "-advantage * total_logprob = -0.4 - -4.975960731506348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.9904, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Compute REINFORCE loss:  L = -A * sum_t log pi(a_t | s_t) ---\n",
    "total_logprob = 0.0\n",
    "for t in range(T):\n",
    "    # sumamos todos los logprobs de los tokens elegidos\n",
    "    logits_t = model(I[t])          # (vocab,)\n",
    "    logp_t  = log_softmax(logits_t) # (vocab,)\n",
    "    tok = chosen_tokens[t]\n",
    "    total_logprob = total_logprob + logp_t[tok]\n",
    "    print(f't={t}: token {tok} logprob={logp_t[tok]} total_logprob={total_logprob}')\n",
    "print(f'-advantage={-advantage}\\n-advantage * total_logprob = {-advantage} - {total_logprob}')\n",
    "loss = - advantage * total_logprob\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c798d9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0207,  0.0412, -0.0441],\n",
       "        [ 0.0472,  0.2640, -0.0441],\n",
       "        [ 0.0930,  0.2376, -0.0941],\n",
       "        [-0.0219, -0.0392,  0.0631],\n",
       "        [ 0.1794, -0.1253,  0.1386]], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Backprop + step\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "opt.step()\n",
    "\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2c443e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-step chosen token probabilities (before -> after):\n",
      "  t=0: token=2   P_before=0.1737 -> P_after=0.2055   (‚Üë)\n",
      "  t=1: token=1   P_before=0.2043 -> P_after=0.2386   (‚Üë)\n",
      "  t=2: token=4   P_before=0.1945 -> P_after=0.2280   (‚Üë)\n"
     ]
    }
   ],
   "source": [
    "# --- Forward AFTER update: record chosen tokens' log-probs and probs ---\n",
    "with torch.no_grad():\n",
    "    after = []\n",
    "    for t in range(T):\n",
    "        logits_t = model(I[t])          # (vocab,)\n",
    "        logp_t  = log_softmax(logits_t) # (vocab,)\n",
    "        tok = chosen_tokens[t]\n",
    "        after.append((tok, float(logp_t[tok]), float(logp_t.exp()[tok])))\n",
    "\n",
    "print(\"Per-step chosen token probabilities (before -> after):\")\n",
    "for t, ((tok_b, logpb, pb), (tok_a, logpa, pa)) in enumerate(zip(before, after)):\n",
    "    assert tok_b == tok_a\n",
    "    arrow = \"‚Üë\" if pa > pb else (\"‚Üì\" if pa < pb else \"‚Üí\")\n",
    "    print(f\"  t={t}: token={tok_b}   P_before={pb:.4f} -> P_after={pa:.4f}   ({arrow})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa8513b",
   "metadata": {},
   "source": [
    "## PPO\n",
    "\n",
    "Limita cuanto se puede cambiar la distribucion para que no hayan pasos demasiado grandes y colapse.\n",
    "\n",
    "Hay un ratio por token\n",
    "\n",
    "$$r_t = \\frac{\\pi_{\\text{nueva}}(a_t \\mid s_t)}{\\pi_{\\text{vieja}}(a_t \\mid s_t)}$$\n",
    "\n",
    "Que te dice cuanto cambiaste la prob para un token. Si es 1.3, subiste 30% la probabilidad para ese token. Si es 0.7 bajaste 30% la prob para ese token.\n",
    "\n",
    "- **Clipping**: Si el paso te lleva a un r_t demasiado grande > 1 + eps, entonces recorto el beneficio\n",
    "- **KL divergence a un modelo referencia**: Penaliza alejarte del modelo base de referencia (el original) en promedio.\n",
    "- **Ventaja por paso** (opcional): en vez de tener un unico A global, lo hacemos por paso, con V(s_t) para asignar mejor el credito y premiar a los tokens clave. Si no esta, se puede usar como baseline la media del batch.\n",
    "- **Entropia**: bonus que mantiene la exploracion y evita colapso de entropia (que se vuelva modo unico)\n",
    "\n",
    "**PPO Loss**:\n",
    "\n",
    "$$L = \\mathbb{E}\\left[ \\min\\left(r_t A_t, \\; \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A_t \\right) \\right] - \\beta \\, \\mathrm{KL}(\\pi_{\\text{new}} \\| \\pi_{\\text{ref}}) + \\alpha \\, \\text{Entrop√≠a}$$\n",
    "\n",
    "Tres grandes partes:\n",
    "\n",
    "- Politica (clipped surrogate objective) -> $\\mathbb{E}\\left[ \\min\\left(r_t A_t, \\; \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A_t \\right) \\right]$\n",
    "- KL -> $- \\beta \\, \\mathrm{KL}(\\pi_{\\text{new}} \\| \\pi_{\\text{ref}})$\n",
    "- Entropia -> $\\alpha \\, \\text{Entrop√≠a}$\n",
    "\n",
    "\n",
    "**Gradiente**:\n",
    "\n",
    "- Lo mismo que REINFORCE, pensamos: Queremos minimizar esta loss. Como hacemos? Que depende de nosotros? los theta. Y donde estan, en pi_new. Todo lo demas es constante porque no depende de nosotros.\n",
    "- Donde esta pi_new? En el ratio: \n",
    "$$r_t = \\frac{\\pi_{\\text{new}}(a_t \\mid s_t)}{\\pi_{\\text{old}}(a_t \\mid s_t)} = \\exp\\left( \\log \\pi_{\\text{new}} - \\log \\pi_{\\text{old}} \\right)$$\n",
    "- Por eso al derivar, todo fluye por pi_new.\n",
    "- PROPIEDAD por la exp: la derivada del ratio es el propio ratio multiplicado por la derivada del log-prob nuevo:\n",
    "\n",
    "$$\\nabla_\\theta r_t = r_t \\cdot \\nabla_\\theta \\log \\pi_{\\text{new}}(a_t \\mid s_t)$$\n",
    " \n",
    "$$\\nabla_\\theta (\\exp\\left( \\log \\pi_{\\text{new}} - \\log \\pi_{\\text{old}} \\right)) = (\\exp\\left( \\log \\pi_{\\text{new}} - \\log \\pi_{\\text{old}} \\right)) \\cdot \\nabla_\\theta \\log \\pi_{\\text{new}}$$\n",
    "\n",
    "Esto se da porque cuando tenemos una funcion exp:\n",
    "\n",
    "$y = \\exp(g(\\theta))$\n",
    "\n",
    "Al derivar exp, te devuelve exp otra vez, y despues multiplicas por la derivada de lo de adentro (**chain rule**):\n",
    "\n",
    "$\\nabla_\\theta y = y \\cdot \\nabla_\\theta g(\\theta)$\n",
    "\n",
    "- pi_old no esta porque no depende de theta, es constante -> $\\exp(\\log \\pi_{\\text{new}} - \\text constante) $\n",
    "- Entonces, cuando r_t = 1 (al principio cuando las dos policies son iguales), entonces el gradiente es igual a REINFORCE!\n",
    "\n",
    "**Politica (clipped surrogate objective)**:\n",
    "\n",
    "En REINFORCE tenemos A_t que es un empuje al gradiente: $A_t \\cdot \\nabla_\\theta \\pi$:\n",
    "- Grad de log pi: es la flecha (vector de cambios) que te dice como mover los pesos para aumentar el valor de la funcion.\n",
    "- A: Empuja a favor de subir o bajar la prob, segun el reward. Le puede cambiar el signo (si fue malo) y cambiarle intensidad.\n",
    "    - Si A > 0: queremos subir la prob\n",
    "    - Si A < 0: queremos bajar la prob\n",
    "\n",
    "**Ratio pi_new/pi_old: Importance sampling**\n",
    "\n",
    "Por que multiplicamos por el ratio? IMPORTANCE SAMPLING (es una correccion estadistica).\n",
    "\n",
    "Las trayectorias se recolectaron con pi_old (porque es caro volver a samplear), pero quer√©s optimizar como si vinieran de pi_new. Estamos trabajando con samples **out of distribution (off-policy)** -> Si bien PPO medio que se clasifica como on-policy en general, esta parte es off-policy.\n",
    "\n",
    "El verdadero gradiente que quiero estimar:\n",
    "$$\\mathbb{E}_{\\tau \\sim \\pi_{\\text{new}}} \\left[ \\sum_{t} \\nabla_\\theta \\log \\pi_{\\text{new}}(a_t \\mid s_t) A_t \\right]$$\n",
    "Esto es lo ideal. Es la esperanza bajo la POLITICA NUEVA.\n",
    "\n",
    "Lo que tenemos en la practica:\n",
    "- Los datos vienen de pi_old. O sea, que si queremos el expectation (dandole mas o menos peso)\n",
    "\n",
    "Lo que queremos es CORREGIR LA ESPERANZA.\n",
    "- Un promedio comun es: 1/N sum(x_i) -> cada ejemplo pesa igual. Esto es igual a: sum(x_i * 1/N)\n",
    "- Un promedio ponderado es: sum(x_i*w_i) / sum(w_i) -> para que sea promedio (suma de pesos = 1). NORMALIZADO.\n",
    "- En IMPORTANCE SAMPLING queremos el promedio COMO SI viniera de otra distribucion. \n",
    "    - Se suele usar version no normalizada por propiedad estadistica: E_q(r(x)*f(x)) = E_p(f(x)) cuando el ratio esta calculado exacto p(x)/q(x)\n",
    "    - Si tomamos muestras de q, multiplicamos cada valor por r y hacemos el promedio simple 1/N, ya recuperamos la expectativa bajo p: (no hace falta dividir por sum(w_i)). Quedaria: **1/N sum(w_i * x_i)**\n",
    "    - Por eso en PPO vemos r dentro de la expectation! Es simplemente la ponderacion de cada \n",
    "\n",
    "Por que? Porque si hicieramos 1/N, le estamos dando importancias distintas a trayectorias. Le damos la importancia de pi_old, que fue desde donde sampleamos, cuando en realidad le queremos dar la importancia de pi_new. IS es promediar con pesos w en vez de 1/N, para que tus muestras (que vienen de la distribuci√≥n vieja) imiten el promedio ‚Äúbajo la nueva‚Äù. Listo. Eso es IS.\n",
    "\n",
    "En PPO:\n",
    "- Tenemos dos expectations/promedios: outer (entre ejemplos (trayectorias) del minibatch), inner (entre tokens dentro de trayectoria). Este ultimo a veces se lo pone como suma nada mas.\n",
    "- Lo ideal seria ponderar por trayectoria completa, o sea en el outer. El tema es que no queda muy bien porque habria que hacer el producto de los ratio...\n",
    "- Se suele **ponderar por token** para aproximar a esa ponderacion por trayectoria. Por eso es que vemos el ratio pi_new/pi_old dentro del inner expectation.\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_{\\text{old}}} \\left[ \\mathbb{E}_{\\text{tokens} \\in \\tau} \\left[\n",
    "\\frac{\\pi_{\\text{new}}(a \\mid s)}{\\pi_{\\text{old}}(a \\mid s)} \\nabla_\\theta \\log \\pi_{\\text{new}}(a \\mid s) A\n",
    "\\right] \\right]\n",
    "$$\n",
    "\n",
    "O solo con suma, como se suele hacer a veces:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_{\\text{old}}} \\left[ \\sum_{(s,a) \\in \\tau} \\frac{\\pi_{\\text{new}}(a \\mid s)}{\\pi_{\\text{old}}(a \\mid s)} \\nabla_\\theta \\log \\pi_{\\text{new}}(a \\mid s) A \\right]\n",
    "$$\n",
    "\n",
    "Efectos:\n",
    "- A la vez de empujarlo con A_t, tambien lo empujamos por el ratio.\n",
    "- Esto es porque vimos que el grad queda: $\\nabla_\\theta r_t = r_t \\cdot \\nabla_\\theta \\log \\pi_{\\text{new}}$\n",
    "- Entonces, los empujes PPO quedarian: $A_t \\cdot r_t \\cdot \\nabla_\\theta \\log \\pi_{\\text{new}}$\n",
    "- Entonces, APARTE DEL **A_t**, ahora tambien lo escalamos por **r_t**: \n",
    "    - Si r_t == 1: queda igual a REINFORCE\n",
    "    - Si r_t > 1: la pi_new ya hace mas probable esa accion que pi_old.\n",
    "    - Si r_t < 1: la pi_new ya hace menos probable esa accion que pi_old.\n",
    "- EFECTOS DE ESCALAMIENTO DE GRADIENTES (A_t): \n",
    "    - Si A > 0, queremos aumentar la prob de esa accion.\n",
    "    - Si A < 0, queremos bajar la prob de esa accion.\n",
    "\n",
    "\n",
    "\n",
    "**CLIP en PPO**:\n",
    "Importance sampling puede dar pesos extremos (alta varianza). PPO mantiene la correcci√≥n pero pone guardarra√≠les: si r_t se va fuera de [1-œµ, 1+œµ] en la direcci√≥n que ‚Äúconviene‚Äù a A_t, corta el empuje (grad = 0 para ese token en ese minibatch).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**FLUJO**\n",
    "\n",
    "Los policies new NO SAMPLEAN. Lo unico que samplea es el OLD (se va actualizando cada tanto).\n",
    "\n",
    "- Tenemos un modelo pi_old. Tenemos prompts. Creamos minibatches (por ejemplo 4 prompts en cada minibatch) y tenemos 3 minibatches (12 ejemplos en total). \n",
    "- Sampleamos la respuesta usando pi_old y guardamos toda la info (probs de los tokens para CADA step, reward, etc) para cada prompt en todos los minibatches.\n",
    "- Ahora empieza la primera corrida de varios EPOCH (1 epoch = 1 pasada por todos los minibatches). Esto se va a hacer SIN HACER NINGUN SAMPLEO A MODELOS. Solo con los datos que estan.\n",
    "- Agarramos el primer minibatch de 4 ejemplos. Para cada ejemplo ya tenemos la secuencia/respuesta originada por el pi_old. Agarramos el primer ejemplo y vamos token por token de la secuencia guardad por el pi_old y vamos comparando los tokens seleccionados con la prob que arroja el modelo (o sea vamos usando el modelo pi_new para ver la prob que arroja para los tokens muestrados originalmente por el pi_old). Obviamente en la primera vuelta va a ser igual o casi igual que el pi_old porque es el mismo modelo, todavia no se actualizo. Usamos la formula de PPO, sumamos para cada token en la secuencia. -> **El advantage despues lo veo bien**\n",
    "- Lo mismo para cada ejemplo del batch. Y promediamos para cada ejemplo del batch.\n",
    "- Despues de cada minibatch, backprop y optimizamos!\n",
    "- Seguimos con los minibatches hasta terminar todos, hasta ahi 1 epoch\n",
    "- Hacemos los varios epochs lo mismo. OJO, el pi_new va acumulando los cambios PERO NO VA SAMPLEANDO.\n",
    "- DESPUES DE ESO, volvemos a samplear pero con el pi_new que reemplaza al pi_old.\n",
    "\n",
    "Si bien no sampleamos nuevas secuencias con los pi_new a medida que van cambiando, los vamos usando activamente para calcular la prob de cada token de la secuencia original no? O sea tienen un uso muy alto. \n",
    "A su vez,es como que los cambios en pi_new se van acumulando durante cada minibatch y eso por varios epochs no? O sea, acumulamos cambios y despues de varios epochs ahi lo usamos para generar una respuesta nueva?\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Advantage**\n",
    "\n",
    "Se hace con actor-critic.\n",
    "\n",
    "aunque en un episodio ves un solo reward, el cr√≠tico se entrena en muchos episodios distintos, y aprende a estimar cu√°nto se espera desde cada paso en promedio. Esa expectativa por estado es lo que te permite calcular ventajas por token.\n",
    "\n",
    "El cr√≠tico trata de aprender una expectativa por estado, no un n√∫mero fijo por episodio.\n",
    "- En el estado justo antes del √∫ltimo token, el cr√≠tico aprende a decir: ‚Äúsi sigo desde ac√°, en promedio espero 0.7 de reward‚Äù.\n",
    "- En un estado m√°s temprano (cuando reci√©n arrancaste a escribir), el cr√≠tico podr√≠a decir: ‚Äúdesde ac√°, en promedio espero 0.2 de reward, porque todav√≠a es muy f√°cil equivocarse‚Äù.\n",
    "- O sea: el cr√≠tico no ve solo ‚Äúun reward concreto‚Äù, sino que aprende el promedio esperado desde cada posici√≥n a lo largo de muchos episodios distintos. \n",
    "- Se entrena en paralelo con su propio value loss.\n",
    "\n",
    "Como evitar que de mucho advantage a los primeros tokens:\n",
    "1. Descuento (\\gamma<1)\n",
    "   El cr√©dito que llega desde el final se aten√∫a hacia atr√°s: cuanto m√°s lejos est√© el token del reward, menos recibe. (Si (\\gamma=0.99), a 50 pasos ya cay√≥ mucho).\n",
    "\n",
    "2. GAE ((\\lambda))\n",
    "   No propag√°s todo el retorno crudo: us√°s ‚Äúsorpresas‚Äù (\\delta_t) y las filtr√°s con (\\gamma\\lambda).\n",
    "    ‚áí Los primeros tokens reciben poquito y ‚Äúsuavizado‚Äù.\n",
    "\n",
    "3. El cr√≠tico aprende ‚Äúlo esperable‚Äù por estado\n",
    "   Al principio, s√≠, (V(s)) temprano suele ser bajo. Pero tras ver muchos episodios buenos, el cr√≠tico sube su (V(s)) en estados iniciales.\n",
    "   ‚áí El *advantage* temprano deja de ser grande porque realidad ‚Äì expectativa se achica.\n",
    "\n",
    "4. Normalizaci√≥n del advantage\n",
    "   En pr√°ctica se hace A ‚Üê (A ‚àí mean)/std por minibatch\n",
    "   ‚áí Evita outliers que dominen el update (ning√∫n tramo de la secuencia ‚Äúse roba‚Äù todo el gradiente).\n",
    "\n",
    "5. M√°s ‚Äúfrenos‚Äù del policy update\n",
    "\n",
    "   * Clip PPO corta empujes grandes token-a-token si el ratio (r_t) se va.\n",
    "   * KL a modelo de referencia otro freno global.\n",
    "   * M√°scaras y pesos pod√©s no dar cr√©dito al *prompt* o bajar el peso a los primeros tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec9955",
   "metadata": {},
   "source": [
    "**Ejemplo Importance sampling**\n",
    "\n",
    "Simple (por trayectoria o outer loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b6e39f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor real (E_p[f]): 2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# Distribuci√≥n objetivo p y de muestreo q\n",
    "p = {\"A\": 0.2, \"B\": 0.8}   # lo que queremos\n",
    "q = {\"A\": 0.6, \"B\": 0.4}   # de donde muestreamos\n",
    "\n",
    "# f(x) = \"gradiente\" de cada caso\n",
    "f = {\"A\": 10, \"B\": 0}\n",
    "\n",
    "# Valor real esperado bajo p\n",
    "true_expectation = sum(p[x] * f[x] for x in p)\n",
    "print(\"Valor real (E_p[f]):\", true_expectation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92e18247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestras de q:\n",
      "A    620\n",
      "B    380\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Muestreamos desde q\n",
    "N = 1000\n",
    "samples = np.random.choice(list(q.keys()), size=N, p=list(q.values()))\n",
    "print(f\"Muestras de q:\\n{pd.Series(samples).value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da037c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio ingenuo (E_q): 6.2\n"
     ]
    }
   ],
   "source": [
    "# Promedio ingenuo\n",
    "naive_estimate = np.mean([f[x] for x in samples])\n",
    "print(\"Promedio ingenuo (E_q):\", naive_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "997265b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimador IS (‚àë w(x) f(x) / N): 2.067\n"
     ]
    }
   ],
   "source": [
    "# Importance Sampling (peso w = p/q)\n",
    "weights = [p[x]/q[x] for x in samples]\n",
    "# print(f'weights (ratios: p(x)/q(x)): {weights}')\n",
    "is_estimate = np.mean([w * f[x] for w, x in zip(weights, samples)])\n",
    "print(\"Estimador IS (‚àë w(x) f(x) / N):\", round(is_estimate, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2425a",
   "metadata": {},
   "source": [
    "Ejemplo mostrando **diferentes tipos de Important sampling**:\n",
    "- objetivo / ground truth\n",
    "- IS \"bien\" por trayectoria\n",
    "- IS \"aprox\" por token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17da9fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'A': 0.8, 'B': 0.2},\n",
       " {'A': 0.3, 'B': 0.7},\n",
       " {'A': 0.8, 'B': 0.2},\n",
       " {'A': 0.3, 'B': 0.7},\n",
       " {'A': 0.8, 'B': 0.2},\n",
       " {'A': 0.3, 'B': 0.7},\n",
       " {'A': 0.8, 'B': 0.2},\n",
       " {'A': 0.3, 'B': 0.7},\n",
       " {'A': 0.8, 'B': 0.2},\n",
       " {'A': 0.3, 'B': 0.7}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng(0)  # cambi√° la seed si quer√©s\n",
    "\n",
    "# Largo de secuencia (tokens por trayectoria)\n",
    "T = 10\n",
    "\n",
    "ACTIONS = [\"A\", \"B\"]  # mantenemos 2 acciones para simpleza\n",
    "\n",
    "# Pol√≠ticas por paso (pi_old y pi_new). Cada paso t tiene probs para A/B.\n",
    "# Pod√©s editarlas a gusto (asegurate que sumen 1 en cada paso).\n",
    "pi_old = [{\"A\": 0.8, \"B\": 0.2} if t % 2 == 0 else {\"A\": 0.3, \"B\": 0.7} for t in range(T)]\n",
    "pi_new = [{\"A\": 0.5, \"B\": 0.5} if t % 2 == 0 else {\"A\": 0.6, \"B\": 0.4} for t in range(T)]\n",
    "\n",
    "# Advantages por token (pueden ser constantes o por paso)\n",
    "A_t = [ +1.0 if t % 2 == 0 else -0.5 for t in range(T) ]  # ej: alterna +1 y -0.5\n",
    "\n",
    "def ratio_token(t, a):  # r_t = pi_new/pi_old\n",
    "    return pi_new[t][a] / pi_old[t][a]\n",
    "pi_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cddccfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth E_{pi_new}[sum A_t] = 2.5\n"
     ]
    }
   ],
   "source": [
    "def seq_prob(policy, seq):\n",
    "    p = 1.0\n",
    "    for t, a in enumerate(seq):\n",
    "        p *= policy[t][a]\n",
    "    return p\n",
    "\n",
    "def ground_truth_under_new():\n",
    "    # E_{tau ~ pi_new}[sum_t A_t]\n",
    "    # (ac√° el integrando no depende de la secuencia, as√≠ que es la suma A_1+A_2)\n",
    "    return sum(A_t)\n",
    "\n",
    "GT = ground_truth_under_new()\n",
    "print(\"Ground truth E_{pi_new}[sum A_t] =\", GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af12f88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras 3 trayectorias:\n",
      "0: [np.str_('A'), np.str_('A'), np.str_('A'), np.str_('A'), np.str_('B'), np.str_('B'), np.str_('A'), np.str_('B'), np.str_('A'), np.str_('B')] | p_old=0.002529 | p_new=0.000720\n",
      "1: [np.str_('B'), np.str_('A'), np.str_('B'), np.str_('A'), np.str_('A'), np.str_('A'), np.str_('B'), np.str_('B'), np.str_('A'), np.str_('B')] | p_old=0.000068 | p_new=0.001080\n",
      "2: [np.str_('A'), np.str_('A'), np.str_('A'), np.str_('B'), np.str_('A'), np.str_('B'), np.str_('B'), np.str_('B'), np.str_('A'), np.str_('B')] | p_old=0.005901 | p_new=0.000480\n"
     ]
    }
   ],
   "source": [
    "N = 16  # cambi√° N libremente\n",
    "\n",
    "def sample_traj_from_old():\n",
    "    seq = []\n",
    "    p_old = 1.0\n",
    "    p_new = 1.0\n",
    "    for t in range(T):\n",
    "        a = rng.choice(ACTIONS, p=[pi_old[t][\"A\"], pi_old[t][\"B\"]])\n",
    "        seq.append(a)\n",
    "        p_old *= pi_old[t][a]\n",
    "        p_new *= pi_new[t][a]\n",
    "    return seq, p_old, p_new\n",
    "\n",
    "batch = [sample_traj_from_old() for _ in range(N)]\n",
    "\n",
    "print(\"Primeras 3 trayectorias:\")\n",
    "for i, (seq, po, pn) in enumerate(batch[:3]):\n",
    "    print(f\"{i}: {seq} | p_old={po:.6f} | p_new={pn:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54aea700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact IS (traj-level): 6.23579417360512\n"
     ]
    }
   ],
   "source": [
    "# Exact IS (traj-level): W_traj = p_new(seq)/p_old(seq)\n",
    "terms_traj = []\n",
    "for (seq, p_old, p_new) in batch:\n",
    "    W_traj = p_new / p_old                 # producto de ratios por token\n",
    "    inner = sum(A_t)                       # sum_t A_t  (tratamos ‚àálog como 1 para ver s√≥lo pesos)\n",
    "    terms_traj.append(W_traj * inner)\n",
    "\n",
    "est_exact_traj = np.mean(terms_traj)\n",
    "print(\"Exact IS (traj-level):\", est_exact_traj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5ae8fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surrogate (token-level): 3.247767857142857\n"
     ]
    }
   ],
   "source": [
    "terms_token = []\n",
    "for (seq, p_old, p_new) in batch:\n",
    "    inner = 0.0\n",
    "    for t, a in enumerate(seq):\n",
    "        inner += ratio_token(t, a) * A_t[t]   # r_t * A_t  (‚àálog ~ 1 para ilustrar)\n",
    "    terms_token.append(inner)\n",
    "\n",
    "est_surrogate = np.mean(terms_token)\n",
    "print(\"Surrogate (token-level):\", est_surrogate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc7c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa565ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploraci√≥n: Exact IS (traj) vs Surrogate (token) seg√∫n T y N\n",
      "  T   N   GT  Exact_mean  Exact_std  Sur_mean  Sur_std  Exact_bias  Sur_bias\n",
      "  3   3  1.5    1.515346   1.503303  1.462351 0.602518    0.015346 -0.037649\n",
      "  3  10  1.5    1.376032   0.624431  1.485491 0.338125   -0.123968 -0.014509\n",
      "  3  25  1.5    1.478170   0.490886  1.478482 0.212735   -0.021830 -0.021518\n",
      "  3 100  1.5    1.462910   0.215665  1.496237 0.108755   -0.037090 -0.003763\n",
      " 10   3  2.5    3.295577  13.788061  2.750149 1.063058    0.795577  0.250149\n",
      " 10  10  2.5    1.963271   2.152243  2.468437 0.549195   -0.536729 -0.031563\n",
      " 10  25  2.5    2.245605   2.275499  2.545161 0.359974   -0.254395  0.045161\n",
      " 10 100  2.5    2.451338   2.042685  2.482991 0.179396   -0.048662 -0.017009\n",
      " 25   3  7.0    2.679560  10.801364  6.850298 1.713741   -4.320440 -0.149702\n",
      " 25  10  7.0    5.779369  40.098797  6.920982 0.849592   -1.220631 -0.079018\n",
      " 25  25  7.0    4.489134  19.168396  6.966018 0.578169   -2.510866 -0.033982\n",
      " 25 100  7.0    5.469588   9.889997  6.987089 0.317970   -1.530412 -0.012911\n",
      "100   3 25.0    0.020948   0.277347 25.458185 3.435895  -24.979052  0.458185\n",
      "100  10 25.0    0.374406   3.353810 24.921339 1.607511  -24.625594 -0.078661\n",
      "100  25 25.0    5.065392  69.907411 25.030179 1.207282  -19.934608  0.030179\n",
      "100 100 25.0    1.302724  10.549693 25.010955 0.621927  -23.697276  0.010955\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "# === Controls you can edit ===\n",
    "T_list  = [3, 10, 25, 100]   # sequence lengths to try\n",
    "N_list  = [3, 10, 25, 100]   # batch sizes to try\n",
    "reps    = 200                # trials per (T, N)\n",
    "\n",
    "# Define 2-action policies that alternate structure across time\n",
    "def make_policies(T):\n",
    "    # old policy alternates (A-heavy at even t, B-heavy at odd t)\n",
    "    pi_old = [{\"A\": 0.8, \"B\": 0.2} if (t % 2 == 0) else {\"A\": 0.3, \"B\": 0.7} for t in range(T)]\n",
    "    # new policy is more balanced / slightly A-heavy on odd t\n",
    "    pi_new = [{\"A\": 0.5, \"B\": 0.5} if (t % 2 == 0) else {\"A\": 0.6, \"B\": 0.4} for t in range(T)]\n",
    "    return pi_old, pi_new\n",
    "\n",
    "# Advantages per token (can be any shape you want)\n",
    "def make_advantages(T):\n",
    "    # Alternate +1 and -0.5 by time step\n",
    "    return [1.0 if (t % 2 == 0) else -0.5 for t in range(T)]\n",
    "\n",
    "ACTIONS = [\"A\", \"B\"]\n",
    "\n",
    "def sample_traj_from_old(pi_old, pi_new):\n",
    "    seq = []\n",
    "    p_old = 1.0\n",
    "    p_new = 1.0\n",
    "    for t in range(len(pi_old)):\n",
    "        a = rng.choice(ACTIONS, p=[pi_old[t][\"A\"], pi_old[t][\"B\"]])\n",
    "        seq.append(a)\n",
    "        p_old *= pi_old[t][a]\n",
    "        p_new *= pi_new[t][a]\n",
    "    return seq, p_old, p_new\n",
    "\n",
    "def ratio_token(pi_old, pi_new, t, a):\n",
    "    return pi_new[t][a] / pi_old[t][a]\n",
    "\n",
    "rows = []\n",
    "for T, N in product(T_list, N_list):\n",
    "    pi_old, pi_new = make_policies(T)\n",
    "    A_t = make_advantages(T)\n",
    "\n",
    "    # Ground truth bajo pi_new: como A_t no depende de la acci√≥n, es sum(A_t)\n",
    "    GT = float(sum(A_t))\n",
    "\n",
    "    exact_vals = []\n",
    "    sur_vals   = []\n",
    "    for _ in range(reps):\n",
    "        # build one batch\n",
    "        batch = [sample_traj_from_old(pi_old, pi_new) for _ in range(N)]\n",
    "\n",
    "        # Exact IS (trajectory-level): weight = product of ratios (p_new/p_old)\n",
    "        terms_traj = []\n",
    "        for (seq, p_old, p_new) in batch:\n",
    "            W_traj = p_new / p_old\n",
    "            # inner sum over tokens (‚àálog term ~ 1 solo para comparar pesos)\n",
    "            inner = sum(A_t)\n",
    "            terms_traj.append(W_traj * inner)\n",
    "        exact_vals.append(np.mean(terms_traj))\n",
    "\n",
    "        # Surrogate (token-level): sum of r_t * A_t; then average over trajectories\n",
    "        terms_token = []\n",
    "        for (seq, p_old, p_new) in batch:\n",
    "            inner = 0.0\n",
    "            for t, a in enumerate(seq):\n",
    "                inner += ratio_token(pi_old, pi_new, t, a) * A_t[t]\n",
    "            terms_token.append(inner)\n",
    "        sur_vals.append(np.mean(terms_token))\n",
    "\n",
    "    rows.append({\n",
    "        \"T\": T,\n",
    "        \"N\": N,\n",
    "        \"GT\": GT,\n",
    "        \"Exact_mean\": float(np.mean(exact_vals)),\n",
    "        \"Exact_std\": float(np.std(exact_vals, ddof=1)),\n",
    "        \"Sur_mean\": float(np.mean(sur_vals)),\n",
    "        \"Sur_std\": float(np.std(sur_vals, ddof=1)),\n",
    "        \"Exact_bias\": float(np.mean(exact_vals) - GT),\n",
    "        \"Sur_bias\": float(np.mean(sur_vals) - GT),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values([\"T\",\"N\"]).reset_index(drop=True)\n",
    "print(\"Exploraci√≥n: Exact IS (traj) vs Surrogate (token) seg√∫n T y N\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ac2ec0",
   "metadata": {},
   "source": [
    "- Exact trajectory IS = te√≥rico, sin sesgo, pero inviable por varianza (crece mucho con T). Al multiplicar muchos ratios, la **varianza explota** porque hay sub/overflow y el cociente pierde precision.\n",
    "- Token surrogate = pr√°ctico, sesgado, pero con varianza baja y entrenable.\n",
    "\n",
    "Es el t√≠pico trade-off de PPO: preferimos estabilidad aunque perdamos exactitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51422e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49c08451",
   "metadata": {},
   "source": [
    "**PPO simplificado** (solo parte de policy clipped. Sin KL ni entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "24035444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Hiperpar√°metros chiquitos\n",
    "vocab_size = 5\n",
    "T = 3\n",
    "lr = 0.5\n",
    "eps_clip = 0.2   # zona segura [0.8, 1.2]\n",
    "advantage = 0.4  # mismo A para todos los pasos (reward final - baseline)\n",
    "\n",
    "# Modelo lineal sin bias (como el tuyo)\n",
    "model = nn.Linear(T, vocab_size, bias=False)\n",
    "with torch.no_grad():\n",
    "    model.weight.copy_(0.1 * torch.randn(vocab_size, T))\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "# Estados one-hot (t = 0..T-1)\n",
    "I = torch.eye(T)\n",
    "\n",
    "# Elegimos una secuencia de tokens (simula lo que gener√≥ œÄ_old)\n",
    "chosen_tokens = [2, 1, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f91e6100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprob_old por paso: ['-1.7502', '-1.5884', '-1.6374']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 1) \"Foto\" de œÄ_old: guardamos logprob_old en esos (s_t, a_t)\n",
    "# ----------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    logprob_old = []\n",
    "    for t in range(T):\n",
    "        logits_t = model(I[t])          # (vocab,)\n",
    "        logp_t  = log_softmax(logits_t) # (vocab,)\n",
    "        tok = chosen_tokens[t]\n",
    "        logprob_old.append(float(logp_t[tok]))\n",
    "    logprob_old = torch.tensor(logprob_old, dtype=torch.float32)\n",
    "\n",
    "print(\"logprob_old por paso:\", [f\"{x:.4f}\" for x in logprob_old.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bac84040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprob_new BEFORE: ['-1.7502', '-1.5884', '-1.6374']\n",
      "P_new BEFORE      : ['0.1737', '0.2043', '0.1945']\n",
      "r_t BEFORE        : ['1.0000', '1.0000', '1.0000']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 2) BEFORE update: logprob_new y ratio (r_t ‚âà 1 al inicio)\n",
    "# ----------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    before = []\n",
    "    ratio_before = []\n",
    "    for t in range(T):\n",
    "        logits_t = model(I[t])\n",
    "        logp_t  = log_softmax(logits_t)\n",
    "        tok = chosen_tokens[t]\n",
    "        logp_new = logp_t[tok]\n",
    "        r_t = torch.exp(logp_new - logprob_old[t])\n",
    "        before.append((tok, float(logp_new), float(logp_t.exp()[tok])))\n",
    "        ratio_before.append(float(r_t))\n",
    "print(\"logprob_new BEFORE:\", [f\"{x[1]:.4f}\" for x in before])\n",
    "print(\"P_new BEFORE      :\", [f\"{x[2]:.4f}\" for x in before])\n",
    "print(\"r_t BEFORE        :\", [f\"{r:.4f}\" for r in ratio_before])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5c7daf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=0: tok=2  logp_old=-1.7502  logp_new=-1.7502  r_t=1.0000  uncl=+0.4000  clp=+0.4000  picked=min(uncl,clp)\n",
      "t=1: tok=1  logp_old=-1.5884  logp_new=-1.5884  r_t=1.0000  uncl=+0.4000  clp=+0.4000  picked=min(uncl,clp)\n",
      "t=2: tok=4  logp_old=-1.6374  logp_new=-1.6374  r_t=1.0000  uncl=+0.4000  clp=+0.4000  picked=min(uncl,clp)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 3) Loss PPO (s√≥lo pol√≠tica, clipeada)\n",
    "#    L = - mean( min( r_t*A, clamp(r_t)*A ) )\n",
    "# ----------------------------------------------------------\n",
    "advantages = torch.full((T,), fill_value=advantage, dtype=torch.float32)\n",
    "\n",
    "total_unclipped = 0.0\n",
    "total_clipped = 0.0\n",
    "for t in range(T):\n",
    "    logits_t = model(I[t])\n",
    "    logp_t  = log_softmax(logits_t)\n",
    "    tok = chosen_tokens[t]\n",
    "    logp_new = logp_t[tok]\n",
    "    r_t = torch.exp(logp_new - logprob_old[t])\n",
    "    unclipped = r_t * advantages[t]\n",
    "    clipped_r = torch.clamp(r_t, 1.0 - eps_clip, 1.0 + eps_clip)\n",
    "    clipped = clipped_r * advantages[t]\n",
    "    total_unclipped = total_unclipped + unclipped\n",
    "    total_clipped   = total_clipped   + clipped\n",
    "    print(f\"t={t}: tok={tok}  logp_old={logprob_old[t]:+.4f}  logp_new={logp_new:+.4f}  r_t={float(r_t):.4f}  \"\n",
    "          f\"uncl={float(unclipped):+.4f}  clp={float(clipped):+.4f}  picked=min(uncl,clp)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5dffdb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Surrogate (mean): +0.400000  ->  PPO policy loss = -0.400000\n"
     ]
    }
   ],
   "source": [
    "# Promediamos el surrogate (min) y lo negamos para minimizar\n",
    "surrogate = torch.minimum(total_unclipped, total_clipped) / T\n",
    "loss = -surrogate\n",
    "print(f\"\\nSurrogate (mean): {float(surrogate):+.6f}  ->  PPO policy loss = {float(loss):+.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f4d42262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backprop + step\n",
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62effa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-step chosen token probabilities (before -> after):\n",
      "  t=0: token=2   P_before=0.1737 -> P_after=0.1839   (‚Üë)\n",
      "  t=1: token=1   P_before=0.2043 -> P_after=0.2153   (‚Üë)\n",
      "  t=2: token=4   P_before=0.1945 -> P_after=0.2052   (‚Üë)\n",
      "r_t AFTER         : ['1.0583', '1.0540', '1.0552']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 4) AFTER update: ver c√≥mo cambiaron P_new de los tokens elegidos\n",
    "# ----------------------------------------------------------\n",
    "with torch.no_grad():\n",
    "    after = []\n",
    "    ratio_after = []\n",
    "    for t in range(T):\n",
    "        logits_t = model(I[t])\n",
    "        logp_t  = log_softmax(logits_t)\n",
    "        tok = chosen_tokens[t]\n",
    "        logp_new = logp_t[tok]\n",
    "        r_t = torch.exp(logp_new - logprob_old[t])\n",
    "        after.append((tok, float(logp_new), float(logp_t.exp()[tok])))\n",
    "        ratio_after.append(float(r_t))\n",
    "\n",
    "print(\"\\nPer-step chosen token probabilities (before -> after):\")\n",
    "for t, ((tok_b, logpb, pb), (tok_a, logpa, pa)) in enumerate(zip(before, after)):\n",
    "    assert tok_b == tok_a\n",
    "    arrow = \"‚Üë\" if pa > pb else (\"‚Üì\" if pa < pb else \"‚Üí\")\n",
    "    print(f\"  t={t}: token={tok_b}   P_before={pb:.4f} -> P_after={pa:.4f}   ({arrow})\")\n",
    "\n",
    "print(\"r_t AFTER         :\", [f\"{r:.4f}\" for r in ratio_after])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc1dea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519ba93c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e78aaac",
   "metadata": {},
   "source": [
    "## GRPO\n",
    "\n",
    "Es similar a PPO pero cambia en el **calculo del Advantage**.\n",
    "\n",
    "Cantidad de trayectorias originales por prompt:\n",
    "- En **PPO** teniamos varios prompts y una sola respuesta original por prompt. Luego ibamos modificando pi_new prediciendo sobre esa misma trayectoria generada original (una por cada prompt). \n",
    "- Ahora en **GRPO** tenemos **k respuestas por prompt**. Tambien vamos modificando pi_new prediciendo sobre las trayectorias generadas originales (k por prompt).\n",
    "\n",
    "Minibatches:\n",
    "- En PPO, si en el batch total tenemos 256 prompts, generamos 256 trayectorias y, si el batch_size=16, nos quedaban 16 mini batches\n",
    "- En GRPO, si en el batch total tenemos 256 prompts, generamos 256*k trayectorias (si k=4 -> 1024 trayectorias). Y si batch_size=16, nos quedan 64 mini batches. O sea, en un minibatch pueden haber dos o mas casos del mismo prompt pero con distintas trayectorias originales.\n",
    "\n",
    "**Baseline y Advantage**:\n",
    "- En PPO se calculaba con un critic o value function. Dos formas: TD advantage por paso o GAE (lo clasico). Involucraba otro modelo aprendible lo cual no era bueno.\n",
    "- En GRPO, tenemos 256 prompts. Para cada prompt generamos k=4 trayectorias -> eso es un GRUPO. Y esas generan rewards. Tomamos la media y desvio de las rewards del grupo y ese es el **baseline**.\n",
    "    - Lo malo es que ahora no tenemos Advantage por token, solo advantage por trayectoria.\n",
    "\n",
    "$$A_i = r_i - \\bar{r}_{\\text{prompt}} \\text{ ------ (y opcionalmente} A_i \\leftarrow \\frac{A_i}{\\mathrm{std}}\\text{)}$$  \n",
    "\n",
    "**Objetivo**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55f875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6440781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874cb47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d096a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83985a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40c401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb90c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
